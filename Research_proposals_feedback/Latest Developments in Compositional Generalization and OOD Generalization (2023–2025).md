# Latest Developments in Compositional Generalization and OOD Generalization (2023–2025)

### 1. Fundamental Computational Capabilities for Compositional Generalization

This section covers recent research into the core abilities needed for models to generalize compositionally rather than merely memorize patterns. Key issues include whether neural networks can bind variables and represent rules in a systematic way, and how to distinguish true compositional reasoning from surface-level interpolation. Several 2023–2025 works explore these questions, often revealing both progress and persistent limitations.

### Does CLIP Bind Concepts? Probing Compositionality in Large Image Models (2024)

**Authors:** Martha Lewis, Nihal Nayak, Peilin Yu, Jack Merullo, Qinan Yu, Stephen Bach, Ellie Pavlick
**Venue & Date:** Findings of ACL (EACL 2024) – March 2024
**Relevance:** Investigates a fundamental capability – *variable binding* – in a multimodal model (OpenAI’s CLIP), testing whether it truly composes concepts or just memorizes visual-text associations. This directly addresses whether current large models possess the primitive for systematic compositional reasoning.
**Key Contributions:**
* Provides **new evaluation datasets** (three synthetic visual tasks) specifically designed to test concept compositionality and binding in vision-language models.
* Demonstrates that **CLIP can compose simple concepts** (e.g. color + object) in trivial settings, *but fails at relational binding* (e.g. distinguishing “A behind B” vs “B behind A”).
* Shows that specialized compositional distributional semantics models (CDSMs) also struggle, performing only at chance level on the binding tasks.

⠀**Technical Approach:** The authors compare CLIP’s representations against those from several compositional semantics models by evaluating on controlled image datasets. Each dataset isolates different aspects: single objects (no binding needed), two-object scenes requiring attribute binding, and relational scenes requiring role-binding. CLIP’s and other models’ accuracies are measured on identifying correct descriptions of images.
**Results & Evaluation:** CLIP performs well when composition is trivial (single-object scenarios) but **drops to near-chance on tasks requiring binding** of attributes or relations. No evaluated model (including neuro-symbolic baselines) showed robust human-like binding ability – the best achieved only chance-level accuracy in relational tests. This indicates current models rely on shallow correlations and fail to represent bindings explicitly when needed.
**Implications:** The findings highlight a *critical limitation* in a state-of-the-art multimodal model: the lack of a fundamental compositional primitive (variable binding). It suggests that without new architectural mechanisms or representations, neural models may not achieve true systematic generalization. This negative result reinforces the need for research into explicit binding mechanisms or neuro-symbolic integration (themes addressed in later sections).
**Code/Data:** *Yes –* The authors released their evaluation datasets and code (via the ACL Anthology supplementary material), enabling reproducibility of the compositional probing tasks.

### How Do Transformers Learn Variable Binding in Symbolic Programs? (2025)

**Authors:** Yiwei Wu, Atticus Geiger, Raphaël Millière
**Venue & Date:** arXiv preprint (May 2025) (under review)
**Relevance:** Probes the *emergence of variable binding* inside a standard Transformer trained on synthetic programs. This work directly examines a hypothesized fundamental capability – whether a neural network *without explicit symbolic pointers* can develop an internal mechanism to bind variables to values. It speaks to the foundational question of whether neural nets can implement symbolic-like operations needed for compositional generalization.
**Key Contributions:**
* Introduces a **training task for binding**: the Transformer is trained to “dereference” variables in synthetic programs, which requires following chains of variable assignments up to 4 steps deep through distractors. This setup forces the model to learn to keep track of variable–value mappings dynamically.
* Identifies a **three-phase learning trajectory** in the model’s training: (1) an initial guess phase, (2) a shallow heuristic phase (e.g. focusing only on early assignments), and (3) the emergence of a *systematic binding mechanism* that correctly handles multi-hop references.
* Uses *causal intervention analysis* to **uncover the learned binding mechanism**: the study finds that the Transformer learns to use its residual stream as an addressable memory and to dedicate specialized attention heads for routing information between token positions. In effect, the model invents a form of pointer mechanism internally.

⠀**Technical Approach:** The authors trained a two-layer Transformer on a custom program execution task where each program assigns variables either constants or other variables, and the model must output the value of a queried variable. By analyzing model behavior and performing interventions (e.g. patching activations, ablating heads), they traced *how* the model stores and propagates variable bindings. They also tracked performance across training to identify distinct learning phases.
**Results & Evaluation:** In the final phase, the Transformer achieved **accurate dereferencing on unseen programs**, demonstrating it had learned a general algorithm for variable binding. Crucially, causal probes revealed that specific attention heads acted like pointers – writing and reading variable values in the residual stream, which serves as a scratchpad memory. This shows that *even without explicit symbolic architecture, a Transformer can implement a binding operation* by repurposing its internal components.
**Implications:** This work provides a **positive mechanistic insight**: Transformers are capable of developing *symbolic-like behavior (pointer-based binding)* given the right training regime. It bridges the connectionist–symbolic debate by showing how a neural network can approximate a key symbolic computation (variable binding) internally. For researchers, it suggests that **architectural support for binding** might be replicated by training dynamics, though possibly requiring careful tasks or inductive biases. It also offers interpretability: understanding these learned circuits could inform new designs that bake in such capabilities from the start.
**Code/Data:** *No official code link* is cited in the paper text, but the problem setup is fully described. (An interactive demo was mentioned and the training configurations are provided, so interested researchers can reimplement the experiments.)

### Relational Composition in Neural Networks: A Survey and Call to Action (2024)

**Authors:** Martin Wattenberg, Fernanda B. Viégas
**Venue & Date:** arXiv preprint (July 2024)
**Relevance:** This is a comprehensive survey that explores *how neural networks might represent and combine structured information*. It’s highly relevant to fundamental capabilities because it catalogs proposed mechanisms (spanning decades) for **binding, composition, and relational representation** in neural systems. It identifies gaps between current neural representations and the needs of true compositional generalization, effectively highlighting which computational primitives might be missing.
**Key Contributions:**
* Reviews a wide range of **mechanisms for relational composition** in neural networks, including vector symbolic architectures, tensor product representations, address-based memory (pointer) systems, and more. It serves as a bridge between older theoretical ideas (dating to Fodor & Pylyshyn and beyond) and modern interpretability findings.
* **Analyzes pitfalls** of current feature-based interpretation methods when facing compositional representations. The authors coin issues like *“feature multiplicity”* (combinatorial mechanisms producing multiple feature vectors for one concept) and *“dark matter”* (information encoded by reference that is invisible to linear probes). These highlight how standard “bag-of-feature” views of neural activations fail for structured information.
* Ends with a “call to action,” outlining **promising areas for empirical research** to determine how (or if) neural nets implement structures beyond superposition of features. This provides a roadmap for developing or discovering the necessary computational primitives (like explicit binding vectors or slot-based architectures).

⠀**Technical Approach:** This is a scholarly survey rather than an experiment. The authors synthesize results from numerous papers across interpretability, cognitive science, and neuro-symbolic AI. They structure the discussion by types of composition mechanisms (e.g. additive binding vs. multiplicative binding, multi-token relations, vector symbolic methods). They illustrate abstract concepts with simple examples and highlight connections to recent studies (including 2022–2024 works on mechanistic interpretability that found evidence of *“binding subspaces”* in transformer activations).
**Findings & Insights:** While not an experimental result, the survey’s insight is that **current neural networks likely do employ some relational composition internally, but it’s hard to detect**. For instance, it discusses evidence of *ID vectors* in networks (unique identifiers for entities used to bind attributes) which create representational “dark matter” – features not directly tied to any input semantic but crucial for relationships. The survey suggests that acknowledging and designing for such hidden structures (e.g. via architecture or regularization) is crucial for progress.
**Implications:** For our research, this survey underscores *which fundamental operations might need to be built-in*. If models are to invent new distributions (extrapolate), they may require capabilities like **explicit variable slots, compositional operators (e.g. tensor products), or reference pointers** – all highlighted in this paper. It also warns that evaluating compositional generalization might require looking beyond surface accuracy; we should use interpretability tools to see if the model is truly representing rules or just bundling features in uninterpretable ways. In short, it encourages us to incorporate lessons from these mechanisms when designing next-generation architectures.
**Code/Data:** *Not applicable.* (This is a review paper. It does, however, cite numerous open-source works and provides references for implementations of various mechanisms.)

### Deep Neural Networks and Humans Both Benefit from Compositional Language Structure (2024)

**Authors:** Lukas Galke, Yoav Ram, Limor Raviv
**Venue & Date:** *Nature Communications*, published 30 Dec 2024
**Relevance:** This study examines whether *compositional input structure* confers a generalization advantage to neural networks similar to the way it helps humans. It addresses a fundamental question: aside from model architecture, does the **structure of the data** (language) affect systematic generalization? The work provides evidence that more compositional languages yield better learning and extrapolation in both humans *and* neural models. This speaks to fundamental capabilities by suggesting that part of the solution may lie in the training distribution’s structure.
**Key Contributions:**
* Conducts **parallel experiments on humans and neural networks** using constructed languages that vary in compositional transparency (from highly compositional/regular to very irregular).
* Shows that both human learners and deep networks (including a large language model and RNNs) **learn faster and generalize more systematically** with a more compositional language input. For humans this replicates known psycholinguistic results, and for neural nets it demonstrates a similar trend, which had been unclear before.
* Finds that networks trained on more compositional languages exhibit **less memorization and more rule-like behavior**: e.g. agents converged on consistent labels for novel meanings and showed better agreement with each other, mirroring human behavior.

⠀**Technical Approach:** The authors created multiple artificial “mini-languages” with controlled degrees of compositionality (e.g., mapping meanings to word forms either transparently or opaquely). Human participants were taught these languages (in a lab experiment), and in parallel neural models were trained on the same mapping tasks. The performance on held-out combinations of meanings tested systematic generalization. The networks tested included a pretrained LLM (for zero-shot and few-shot learning) and trained LSTM-based agents interacting in a referential game.
**Results & Evaluation:** Both humans and neural models showed a **“compositionality advantage”**. For example, with a highly compositional language, an LSTM achieved significantly higher generalization accuracy on novel combinations than with an irregular language. Humans likewise learned the compositional language faster and generalized better. Interestingly, the large pretrained model also performed better zero-shot on tasks expressed in a more regular compositional form. These results confirm that compositional structure in data can improve model generalization *without changes to architecture*.
**Implications:** This work suggests that one fundamental ingredient for generalization might lie in *the training data/experience itself*. In our context, it underscores the importance of **training protocols that emphasize systematic relationships** (as opposed to exposing models only to random or entangled data). It aligns with ideas like *curriculum learning* or *iterated learning*, which enforce compositional presentations. While it doesn’t eliminate the need for architectural innovation, it indicates that pairing the right data structure with even standard architectures yields more systematic behavior. Practically, for our research, it might mean designing training curricula or synthetic tasks that encourage rule formation over memorization, thereby equipping models with a more human-like learning bias.
**Code/Data:** *Data & Code Availability:* The study’s design and analysis rely on custom experimental data. While the paper doesn’t explicitly list a code repository in the excerpt, it being a Nature Communications paper implies that data or code might be available as supplementary material. (We should check the publication’s appendix for links if needed.)

### 2. Architectural Innovations Targeting Out-of-Distribution Generalization (2023–2025)

Despite improvements in standard benchmarks, neural networks often falter when tested on novel distributions or compositions not seen in training. This section surveys novel architectures and modeling frameworks proposed in the last two years specifically to tackle **out-of-distribution (OOD) generalization**. These include modular designs that can recombine learned skills, architectures encoding explicit composition operators or causal structure, and approaches leveraging energy-based formulations. We focus on advances beyond conventional Transformer or CNN tweaks – instead highlighting fundamentally different designs aimed at robustness to distribution shift.

### Discovering Modular Solutions that Generalize Compositionally (Schug et al., 2024)

**Authors:** Simon Schug, Seijin Kobayashi, Yassir Akram, Maciej Wołczyk, Alexandra M. Proca, Johannes von Oswald, Razvan Pascanu, João Sacramento, Angelika Steger
**Venue & Date:** *ICLR 2024* (poster), published Jan 2024
**Relevance:** Proposes a meta-learning framework to induce **modular neural architectures** that achieve compositional generalization. Rather than a fixed architecture, the method learns to configure modules for each task, which is directly aimed at solving OOD generalization in tasks with hidden compositional structure. This addresses the focus area of *dynamic modular networks* and explicit composition – the learned modules can be recombined to handle novel task combinations.
**Key Contributions:**
* Introduces a *teacher–student meta-learning setup* where the teacher model is explicitly modular (with known ground-truth modules composing the task), and a student (hypernetwork-based) model must learn to identify and use those modules from data. This setup provides a controlled scenario to study when a neural system can recover compositional structure.
* Provides **theoretical results** showing that under certain conditions, the student hypernetwork can identify the correct modules *up to a linear transformation*, *without* needing exponentially many task combinations. This is important because it demonstrates compositional reuse is possible without exhaustive experience of every combination.
* Empirically demonstrates that with finite meta-training tasks, **meta-learning yields modular policies that extrapolate** to new task compositions in complex environments. For example, in their experiments (including a multi-room navigation domain and others), a meta-trained network could solve novel combinations of sub-tasks by reusing learned building-block policies.

⠀**Technical Approach:** The student model is implemented as a **hypernetwork** – essentially a network that generates the weights of a modular policy network. During meta-training, it sees a variety of tasks (each composed of a set of underlying modules). The objective is set such that the student must quickly adapt (with or without fine-tuning) to new tasks by producing appropriate module combinations. By controlling the tasks the student sees, the authors ensure that at test time it faces *novel compositions* of modules. They derive conditions for identifiability (the student can recover true modules) and test various settings (e.g., with different numbers of modules and levels of noise).
**Results & Evaluation:** Under the theoretically identified conditions – such as sufficient task diversity and an architecture that allows multiplicative interactions (as hypernetworks do) – the method succeeds in discovering the true underlying modules. Empirically, the meta-learned models achieved near-perfect generalization on held-out task combinations, significantly outperforming baselines that lack a modular bias. The approach was validated in **complex environments** (including simulated control tasks) where policies had to be composed; only the modular meta-learned models were able to adapt to new compositions with high success.
**Implications:** This work shows that **architectural modularity + meta-learning** is a powerful combination for OOD generalization. It suggests that if we want a model to invent solutions for novel scenarios (distribution *invention*), we should equip it with a modular design or meta-learning procedure that can leverage modules. Notably, the success conditions point out that certain inductive biases (like hypernetwork-based weight generation enabling *factorized representations*) are crucial. For our research, this means exploring architectures that *prevent entanglement* of features and instead allow recombination. The demonstrated ability to generalize from finite data to exponential task combinations is precisely the kind of result we hope to achieve, indicating this line as a promising direction.
**Code/Data:** *Availability:* As of the publication, no official code link is provided in the openreview page. The experiments are described in detail, though, and likely the authors will release code (Razvan Pascanu and colleagues often do). The paper’s theoretical proofs and pseudocode for the meta-training procedure are included, facilitating reimplementation.

### Out-of-Distribution Generalization via Composition: A Lens through Induction Heads in Transformers (Song et al., 2025)

**Authors:** Jiajun Song, Zhuoyan Xu, Yiqiao Zhong
**Venue & Date:** *PNAS* (Proceedings of the National Academy of Sciences), early 2025 (open-access article)
**Relevance:** This work provides a *mechanistic analysis* linking **transformer architecture features (induction heads)** to OOD generalization capability. Rather than proposing a new architecture, it uncovers that *composition* within an existing architecture (composing multiple self-attention layers) is tied to the model’s ability to infer hidden rules and generalize beyond training distribution. This directly addresses architectural insights: it suggests that even standard Transformers have latent architectural elements that, when utilized (two-layer composition, shared subspace), lead to improved OOD generalization. It’s a novel perspective on *why* certain architectures might succeed out-of-distribution.
**Key Contributions:**
* Shows empirically that a **2-layer Transformer can exhibit emergent OOD generalization** on a synthetic rule-learning task (sequence copying), whereas a 1-layer Transformer cannot. The two-layer model developed a *“subspace matching”* between layers – effectively a shared latent representation that enables rule application in new contexts.
* Proposes the **“common bridge representation” hypothesis**: that OOD generalization in Transformers requires a *shared latent subspace* aligning early and late layers, which allows the model to *compose* intermediate computations into a higher-level rule. This was supported by observing that successful models had strongly correlated principal subspaces between layers.
* Evaluates **pretrained large language models** on tasks requiring inference of hidden rules (including in-context learning scenarios) and finds evidence consistent with the hypothesis: models that succeeded displayed signs of using a common subspace for composition, likely via their induction head circuitry. This indicates the findings scale from toy tasks to real models.

⠀**Technical Approach:** The authors first train small Transformers from scratch on a *copying task* with a distribution shift (the test sequences are longer or with different token frequencies than seen in training). They analyze training dynamics and neuron activations, discovering an abrupt phase transition where the 2-layer Transformer’s layers align and the model begins generalizing (copying unseen lengths perfectly). They attribute this to the induction head mechanism (a known transformer attention pattern that copies sequences). Then, they examine various pretrained GPT-like models on tasks such as inferring a secret cipher or performing reasoning with novel symbols, measuring how well they generalize with just prompts. They connect success cases with the presence of induction heads and representational alignment.
**Results & Evaluation:** On the synthetic tasks, a clear result was that **two self-attention layers composed in sequence can learn a rule that generalizes OOD**, whereas one layer cannot. This composition of layers essentially forms a higher-order operation (an induction head spanning layers) that can copy or implement a rule not explicitly seen. For large models, the paper reports that those known to have strong in-context learning (like GPT-3/4) indeed seem to utilize induction head patterns to perform OOD generalization (like extending a sequence pattern beyond seen length), supporting the idea that *composition = generalization* in these systems.
**Implications:** This work suggests an architectural principle: enabling **layer-wise composition and shared representations** can be key to OOD performance. In practical terms, it means architectures should allow intermediate computations to feed into each other in a compositional way (the success of a 2-layer vs 1-layer attention is a simple but striking example). It also validates the importance of *induction heads*, which are a kind of emergent architectural feature in Transformers, as engines of generalization. For our research, the *common bridge representation* idea resonates with ensuring the model learns an abstract state that multiple modules or layers agree on – akin to a factorized representation that can be reused across contexts. The findings encourage us to inspect and perhaps enforce such properties (e.g. via regularizers that align layer subspaces) in any new architecture we design.
**Code/Data:** *Yes –* The authors have made their code available. The project’s GitHub is linked on the first author’s page, and the arXiv preprint provides pseudocode for key experiments. Additionally, a **blog article** and a **PNAS dataset** of tasks were released alongside the publication, aiding reproducibility and further analysis by others.

### Causal Representation Learning and Inference for Generalizable Cross-Domain Predictions (Yin et al., 2024)

**Authors:** Naiyu Yin, Hanjing Wang, Amit Dhurandhar, Tian Gao, Qiang Ji
**Venue & Date:** *ICLR 2024* (research paper), camera-ready Feb 2024
**Relevance:** Proposes a new framework that integrates **causal inference with representation learning** to achieve robust predictions under distribution shifts. This directly addresses *causal representation learning*, a focus area, by explicitly modeling latent causal variables and intervening to remove spurious correlations. The architecture infers a structured representation (with causal and non-causal parts) such that only the invariant causal part is used for prediction. This innovation targets OOD generalization by design – if the cause-effect structure is correctly captured, the model should generalize to new domains where superficial correlations differ.
**Key Contributions:**
* Introduces a **Structural Causal Model (SCM)** with latent variables underlying the data generation. In this SCM, some latent factors influence the label in an invariant way (causal factors), while others capture domain-specific variations or confounders that change across environments.
* Proposes a **two-step training procedure**: first, perform *causal intervention* in latent space by simulating interventions on the spurious components, effectively *removing their influence*, and second, learn a representation that aligns with the *invariant interventional distribution*. In simpler terms, they generate counterfactual-like data to train the predictor only on causally robust features.
* Demonstrates that their **causal representation learning approach outperforms standard domain generalization methods** on various benchmarks under distribution shift. The approach is shown to be more **robust and generalizable** especially when test data involves new combinations of nuisance factors not seen in training.

⠀**Technical Approach:** The model is composed of an **encoder** that maps inputs to latent factors and a **predictor** that outputs the target. The encoder is structured to produce two sets of latents: one that should contain causal factors and one for non-causal factors. During training, the method performs an *intervention* by resampling or permuting the non-causal latents (simulating a change in environment) while keeping causal latents the same, and then enforcing that the predictor’s output remains stable for the same causal latents. This encourages the model to truly isolate what latent information is causal. They employ techniques from causal inference (like transportability and invariance principles) to guide this process. Empirically, they test on tasks like image classification across different domains (where style is a spurious factor) and show improved OOD accuracy.
**Results & Evaluation:** Compared to state-of-the-art domain generalization baselines (e.g., IRM, CORAL, etc.), the proposed method achieved **higher accuracy on unseen domains** and more stable performance when faced with shifts in non-causal features. For example, on a rotated MNIST experiment or a synthetic multi-domain dataset, their model could maintain low error even when the test domain had unseen confounder values, whereas traditional models failed. Ablation studies indicated that the intervention step was crucial – removing it led to models picking up spurious correlations and collapsing in generalization tests.
**Implications:** This work underscores the power of **causal architectural thinking**: building in a separation between core factors and nuisances. For our goals, it provides a blueprint for how to incorporate *causal priors* into neural architectures. A takeaway is that *explicitly modeling and training for invariance* (via interventions or adversarial schemes) can yield models that don’t get fooled by distribution shifts. Applying this to compositional tasks, we might consider each compositional rule as a causal mechanism and each context or background detail as a confounder, then train the model to isolate the rule. Overall, Yin *et al.*’s approach can inspire hybrid architectures that merge neural nets with causal graphs or factorized latent spaces – aligning with our interest in systematic generalization.
**Code/Data:** *Availability:* The authors have not listed a public repo in the abstract, but since this was an ICLR contribution, it’s likely they have a code release (possibly on the authors’ institutional sites or upon request). The method relies on standard domain generalization datasets (which are publicly available). We should watch for a library or code release named “DFIL (or similar)” corresponding to this paper for direct use.

### Secure Out-of-Distribution Task Generalization with Energy-Based Meta-Learning (EBML) (Chen et al., 2023)

**Authors:** Shengzhuang Chen, Long-Kai Huang, Jonathan R. Schwarz, Yilun Du, Ying Wei
**Venue & Date:** *NeurIPS 2023* (poster), published Sept 2023
**Relevance:** This work combines **energy-based models with meta-learning** to handle OOD tasks in a safety-critical context. It specifically addresses cases where an agent encounters an *entire task from a new distribution* (not just new data points), focusing on detecting such shifts and adapting accordingly. The architecture is novel in that it doesn’t assume the model will perform well OOD by default; instead, it integrates an OOD *detection mechanism* and a subsequent *adaptation mechanism* into one meta-learned energy-based framework. This aligns with interest in energy-based models for OOD and meta-learning for structural changes (here the “structure” is the task’s data distribution).
**Key Contributions:**
* Proposes a **unified framework (EBML)** that handles **both OOD task detection and OOD adaptation** within meta-learning. Traditional meta-learning yields a prior that adapts quickly to new tasks; EBML augments this by first assessing if a new task is out-of-distribution relative to meta-training tasks, using an energy score.
* Uses **two neural energy functions** to model the distribution of tasks: intuitively, one captures the within-task likelihood and the other the task’s divergence from the meta-training distribution. By summing them, they obtain an energy-based *score for a task*. High energy indicates the task is likely OOD.
* Demonstrates on several regression and classification meta-learning benchmarks that EBML can **reliably detect OOD tasks and then improve performance by adaptive training** on them. It outperforms standard Bayesian meta-learning in scenarios where test tasks differ from training tasks (including safety-critical domains where misdetection would be costly).

⠀**Technical Approach:** The meta-learning backbone can be any conventional algorithm (they likely use a gradient-based meta-learner or amortized inference). EBML adds an **energy model** $E_\theta(task)$ which is meta-learned to assign low energy to in-distribution tasks and higher to out-of-distribution ones. During meta-testing, given a new task, EBML first computes the task’s energy. If above a threshold, it flags it as OOD. Then it performs an *adaptation step*: essentially, it adjusts the task’s data or intermediate representations by minimizing the energy with respect to the input (bringing the task closer to the training distribution in representation space). After this “task adaptation,” the meta-learned model (prior) is used to make predictions or fine-tune on the task.
**Results & Evaluation:** The experiments involved four datasets in regression and few-shot image classification where test tasks had covariate or concept shifts relative to training. EBML consistently **improved generalization performance on OOD tasks** compared to baselines like MAML, and crucially, it **successfully identified** when a task was OOD (with high detection accuracy). For example, in a sinusoid regression meta-learning setup, if an OOD task had a frequency outside the training range, EBML would detect it and adjust the input distribution, resulting in significantly lower error after adaptation than a model that either wasn’t aware of the shift or only did standard fine-tuning.
**Implications:** The EBML approach highlights an architecture-level idea: combining **uncertainty/energy-based awareness with adaptation**. It’s not enough to have a model that can learn quickly; for true safety and robustness, the model should *first recognize when it is out of its depth*. In our context, this could translate to mechanisms where a compositional learner first gauges if a new problem involves unseen combinations or concepts and then triggers a special compositional adaptation routine. It aligns with the intuition that *detection of novelty* is a prerequisite to successful extrapolation. Moreover, using energy-based models suggests a way to model task distributions explicitly rather than implicitly – this could be useful if we want the system to reason about whether a new input is within its learned scope or requires inventive reasoning.
**Code/Data:** *Status:* The OpenReview entry did not have a direct link to code. However, given the authors (including researchers from DeepMind and MIT), it’s possible that an implementation might be released. The paper’s pseudocode and detailed algorithmic description would allow re-creation. As for data, they used standard meta-learning benchmarks (e.g., sinusoid regression, MiniImageNet variants), so no new dataset needed.

### 3. Hybrid Neuro-Symbolic Approaches for Systematic Generalization

Hybrid models that combine neural networks with symbolic reasoning components have gained traction as a path toward systematic generalization. By incorporating discrete logic, rules, or program-like structures into neural systems, these approaches aim to get the best of both worlds: the flexibility of learning and the systematicity of symbolic manipulation. In 2023–2025, we’ve seen progress in differentiable rule-learning, neural logic machines, and other neuro-symbolic architectures that achieve new state-of-the-art results on compositional benchmarks. Below we review key papers that illustrate these advances, including models that *explicitly parse and reason* and those that learn logic programs from data.

### Neural-Symbolic Recursive Machine (NSR) for Systematic Generalization (Li et al., 2024)

**Authors:** Qing Li, Yixin Zhu, Yitao Liang, Ying Nian Wu, Song-Chun Zhu, Siyuan Huang
**Venue & Date:** *ICLR 2024* (poster), published Jan 2024
**Relevance:** NSR is a prime example of a *neuro-symbolic architecture* purpose-built to overcome the failures of pure neural nets on compositional tasks. It combines neural perception with **symbolic parsing and reasoning modules** inside a unified system. This directly addresses systematic generalization: by incorporating an explicit *Grounded Symbol System (GSS)*, NSR allows combinatorial syntax and semantics to emerge from data. It squarely targets the benchmarks (SCAN, PCFG, etc.) where conventional models struggled, achieving state-of-the-art generalization.
**Key Contributions:**
* Proposes a novel architecture consisting of **modular components**: a neural perception module (e.g. for feature extraction from input), a **syntactic parser** that converts inputs into symbolic-like expressions, and a **semantic reasoning module** that operates on these expressions. These modules work recursively, enabling the model to build and interpret compositional structures.
* Introduces a **deduction–abduction learning algorithm** for training the system end-to-end. In essence, the model can deduce outputs from inputs using its current symbolic rules, and when it fails, it abductively adjusts its rules and perceptions. This learning process imbues the system with strong inductive biases of equivariance (treating similar substructures similarly) and compositionality.
* Demonstrates **unprecedented performance** on four challenging compositional generalization benchmarks: SCAN (command-based navigation tasks), PCFG (compositional translation for a context-free grammar), HINT (arithmetic reasoning with latent steps), and a new compositional machine translation task. NSR *significantly outperforms* prior neural and neuro-symbolic models, often achieving near-perfect generalization where others were far below 100%.

⠀**Technical Approach:** The *Grounded Symbol System* (GSS) at NSR’s core is essentially a learned discrete language for the task domain. For example, on SCAN, NSR learns an intermediate “syntax” representing commands, and a set of semantic rules (like logical forms) to execute them. The neural perception module maps raw input (e.g. strings or images) into this symbolic form. Then a recursive reasoning module (which could be thought of as a learned logic engine) computes the output. Training is done with backpropagation through a continuous relaxation of the symbolic components or via policy gradients, using the deduction-abduction paradigm to ensure consistency. The inductive biases (equivariance, etc.) are implemented via weight sharing and architectural design that treat symbol permutations appropriately.
**Results & Evaluation:** On SCAN, for instance, NSR achieved **over 90% generalization accuracy** on the challenging splits (where prior best models were in the 50–75% range). On the PCFG productivity challenge (generalizing to longer strings), NSR also excelled, correctly parsing and executing sequences far longer than seen in training. The authors report that NSR not only got the right answers but did so by learning interpretable rules akin to the ground-truth grammar. In the HINT arithmetic dataset, NSR was able to infer intermediate steps (like carry operations) that generalize to new arithmetic expressions, outperforming pure neural seq2seq models by a wide margin. These comprehensive evaluations indicate NSR’s effectiveness across domains (language, math) and modalities.
**Implications:** NSR represents a **paradigm shift**: rather than hoping a black-box network will learn compositionality, it *bakes in a symbolic scaffold*. The success of NSR suggests that incorporating *explicit discrete structure (parsing trees, logic rules)* is a viable path to true extrapolation. For our research, this means exploring hybrid architectures where, for example, a neural network might output a provisional *program* that another module executes. The deduction-abduction training strategy is also noteworthy – it mirrors how one might debug a symbolic reasoning system and could inspire algorithms for adjusting neural parts when symbolic outputs err. NSR’s results on tough benchmarks give hope that the long-standing generalization problems can be solved by marrying neural learning with symbolic reasoning.
**Code/Data:** *Availability:* The authors have not directly linked a repository in the paper text. However, given the significance of the results, it is likely that they will release code (perhaps on Yixin Zhu’s or Song-Chun Zhu’s lab websites). The benchmarks used (SCAN, PCFG dataset from Keysers et al., HINT from Zhang et al.) are publicly available, and the paper’s appendix contains pseudo-code for the algorithm, so reproduction is feasible with effort.

### Differentiable First-Order Rule Learner (DFORL) (Gao et al., 2024)

**Authors:** Kun Gao, Katsumi Inoue, Yongzhi Cao, Hanpin Wang
**Venue & Date:** *IJCAI 2024* (to appear, preprint Jan 2024)
**Relevance:** DFORL is a new **differentiable inductive logic programming** model that learns logical rules from relational data in a robust and scalable way. This falls under *differentiable rule learning*, a key aspect of hybrid approaches. By learning first-order logic rules with gradient-based optimization, DFORL can handle noisy data better than traditional ILP and can scale to larger knowledge graphs, addressing two common limitations of symbolic methods (sensitivity to noise and poor scalability). Its ability to discover human-readable rules that generalize makes it valuable for systematic generalization tasks where underlying relations matter.
**Key Contributions:**
* Develops a **differentiable ILP architecture** that does not require a pre-specified language template or heavy restrictions on rule form (often needed in prior neuro-symbolic ILP approaches). DFORL only needs to know the number of variables to consider in a rule, and it will learn the rule structure and parameters via gradient descent.
* Achieves a **blend of precision and scalability**: it can learn accurate logical rules from both small, noise-free datasets *and* large, noisy knowledge bases. Experiments in the paper show it outperforming both classical ILP systems and other neural ILP methods on benchmarks like family relationship inference and knowledge graph completion.
* Introduces techniques to improve robustness, such as a specially designed loss that tolerates some error in rule predictions (addressing noise) and an architecture that can handle both **small data regime (learning from a handful of examples)** and **large data regime (scaling to thousands of facts)** without changing the formulation.

⠀**Technical Approach:** While details are technical, at a high level DFORL encodes candidate logical rules in a neural form (for example, as a tensor or neural network that approximates the truth table of the rule). It uses **neural attention or gating** to select which predicates and variables appear in the rule, effectively searching through the space of possible rules but in a differentiable manner. The training objective is to satisfy as many provided relational facts as possible while not overfitting to noise. One key novelty is that DFORL doesn’t require users to enumerate rule templates or ground all possibilities; it searches in a high-level space guided by gradients. This dramatically reduces the need for *language bias*, making it more general.
**Results & Evaluation:** On standard ILP benchmarks (like learning family relations “uncle”, “grandparent” from examples, or learning rules in WordNet or knowledge graph datasets), DFORL showed **higher accuracy and recall** than previous approaches. For instance, it could learn rules for a family tree problem with perfect precision and high recall even when 10% of facts were corrupted with noise, where symbolic ILP failed or returned wrong rules. On a large knowledge graph (WN18RR, a common KG completion set), DFORL could learn interpretable rules that achieve strong link prediction performance, rivaling black-box embedding models but with the bonus of interpretability. These results demonstrate the method’s capability to handle both **noise (robustness)** and **scale (thousands of entities)** – a combination that previous differentiable ILP systems struggled with.
**Implications:** For systematic generalization, DFORL offers a path to *learn explicit rules from data*. If our tasks can be represented in a relational form (e.g. object relationships in a scene, or transitions in a reasoning problem), DFORL could potentially infer the underlying rules that govern the system. Such rules are inherently compositional and extrapolatable (logic rules apply to any new entities, not just training ones). The fact that it’s differentiable means it can integrate with neural perception: one could imagine a pipeline where a neural network perceives symbols, and DFORL then learns rules over those symbols – aligning with the “perception + symbolic reasoning” paradigm. The improved noise tolerance is also crucial, as real data (or outputs from learned perception modules) can be messy. Overall, DFORL and similar advances suggest that *neural-symbolic systems can now induce their own symbolic knowledge*, rather than requiring it to be provided, which is a big step toward flexible general intelligence.
**Code/Data:** *Availability:* The authors mention in their OpenReview abstract that this work is open-access under CC BY-SA license, which implies they are willing to share. Indeed, the presence of a GitHub link for an earlier differentiable ILP (αILP by Shindo et al., 2023) suggests that a repository for DFORL might be released (we saw references to *DFOL* in search results). We should keep an eye on Kun Gao’s homepage or GitHub for a DFORL implementation, as using it on our tasks could be very insightful.

### Position: Still No Human-like Systematic Compositionality in Neural Networks (Fodor & Pylyshyn’s Legacy) (Anonymous, 2024)

**Authors:** *Anonymous (under review)* – reportedly M. Nezhurina *et al.* (ICML 2024 submission)
**Venue & Date:** Preprint (ICML 2024 under review), Nov 2024
**Relevance:** While not proposing a new model, this position paper provides a **contrarian perspective** on recent neuro-symbolic claims – specifically a critical evaluation of Lake & Baroni (2023)’s meta-learning approach (which we discussed in Theme 6). The authors argue that even with approaches like MLC, current neural networks have *not* truly achieved human-like systematic compositionality. They identify hidden pitfalls and necessary ingredients that recent papers may have overlooked. This is important for a balanced view: it outlines potential *limitations or missing components* in hybrid approaches that we should be mindful of.
**Key Points:**
* The authors present **specific test cases** within Lake & Baroni’s Meta-Learning for Compositionality (MLC) framework that cause systematic errors, despite MLC’s success on many benchmarks. These adversarial or out-of-scope cases are still trivial for humans but lead the neural model to make “transduction” mistakes, indicating it hasn’t truly internalized a compositional rule in those instances.
* They highlight **“overlooked yet essential elements”** required for genuine systematic generalization. While not all listed in the snippet, this likely includes things like the need for *stronger inductive biases or architectural features* (e.g., variable binding mechanisms, symbolic memory) that even MLC didn’t include, or the need for *explicit compositional representations* rather than just training procedures. Essentially, they argue optimization alone (meta-learning) may not be enough without the right architecture.
* The paper concludes that it is *premature to claim* modern neural architectures have overcome the limitations noted by classic critics like Fodor and Pylyshyn. This directly tempers some of the optimism from recent breakthroughs, asserting that while progress has been made (neural models *mimicking* human behavior on certain tests), the underlying capability might still be lacking or fragile.

⠀**Evidence & Reasoning:** The authors likely designed additional evaluation tasks or variations of the original tasks used by Lake & Baroni. For example, if MLC solved SCAN modifications, this paper might have created a different composition or a more complex combination that MLC fails at (but a symbolic system would handle). They also cite other contemporaneous works (Nezhurina et al. 2024, Wüst et al. 2024, Bayat et al. 2025) which have shown reasoning breakdowns in large language models or meta-learning agents. By aggregating these negative results, the position paper strengthens the argument that current systems are still fundamentally limited.
**Implications:** For our research, this serves as a **cautionary note**. It suggests that even approaches that appear to succeed (like meta-learning, or certain neuro-symbolic models) should be tested rigorously for hidden failure modes. Specifically, we might want to include some of the “class of test cases” identified by these authors when evaluating our models. The paper also implicitly points to *what might be missing* in architectures: perhaps things like **systematicity in representation**, the ability to handle novel primitives (MLC still had a fixed set of primitives), or the need for *meta-reasoning* (knowing when to switch strategies). While hybrid and meta-learning methods are promising, this critique reminds us that **true extrapolation might require innate structure or priors** that aren’t yet fully realized in today’s models.
**Code/Data:** Being a position paper under double-blind review, there’s no code. However, if their test cases are described, we can implement similar evaluations. The broader message is to incorporate such stress tests in our pipeline.

### 4. Evaluation Methodologies: Beyond Standard Benchmarks

Our findings pointed out how conventional evaluation can mask true failures (the “Evaluation Illusion”). In the past two years, researchers have become increasingly aware of this and proposed *improved evaluation methodologies* for compositional and OOD generalization. This section covers works that critique existing benchmarks, compare them, or introduce new ones designed to more rigorously test systematic generalization. We also note efforts to make evaluations more *behavioral* (probing specific capabilities) and the introduction of upgraded challenges like **ARC-AGI-2** for fluid intelligence.

### The Validity of Evaluation Results: Assessing Concurrence Across Compositionality Benchmarks (Sun et al., 2023)

**Authors:** Kaiser Sun, Adina Williams, Dieuwke Hupkes
**Venue & Date:** *CoNLL 2023* (Conference on Computational Natural Language Learning), December 2023
**Relevance:** This paper critically examines **whether popular compositional generalization benchmarks actually agree on what they measure**. It asks: if a model does well on one benchmark (e.g., SCAN), does it do well on others (e.g., COGS, CFQ)? If not, why? This is directly relevant to our evaluation concerns because it shows *standard metrics may be dataset-specific* and not general. By finding misalignment between benchmarks, the authors underscore the need for more robust or unified evaluations. Essentially, it challenges the field to consider *evaluation validity* – are we truly measuring “compositional generalization” or just idiosyncrasies of each dataset?
**Key Findings:**
* **Inconsistency across benchmarks:** The study found that the ranking of models by performance can differ significantly between compositional generalization datasets. A model that is top on one benchmark might be middle-tier on another. This suggests that each dataset might emphasize different aspects of compositionality or have different biases.
* **Human-crafted vs. synthetic data:** They observed that **benchmarks created by humans (with natural language or human-designed tasks) correlate better with each other** in terms of model rankings than benchmarks that are fully synthetic. Synthetic benchmarks (like SCAN, CFQ) each have their own particular definition of splits, which may not align. Human-created ones (like COGS or certain semantic parsing tasks) share more common structure, so model performance is more consistent across them.
* **Source of data > definition of compositionality:** Interestingly, whether two datasets agreed on what constitutes “compositional” generalization mattered less than whether they came from the same source or domain. For example, two semantic parsing tasks might both be natural language-based and thus pose similar challenges to models, even if one tests productivity and another systematicity. This raises a concern that *we might be conflating linguistic generalization with general algorithmic compositionality*.
* **Lexical effects:** The paper also notes that specific *lexical items* (words, symbols) used in a benchmark can impact consistency. If one dataset’s generalization requires knowledge of a particular word or concept and another doesn’t, model performance can diverge. This highlights that some generalization “failures” might be due to vocabulary or knowledge differences rather than inability to compose per se.

⠀**Implications for Evaluation:** The authors conclude that *much work remains to ensure benchmarks measure what we intend*. For our research, this means we should be careful in selecting evaluation tasks: relying on a single benchmark (like SCAN) is not enough. We should test models on multiple datasets and consider diverse types of compositional challenges. We might also need to design new evaluation protocols that target core abilities (like separate tests for systematicity, productivity, etc.) to get a complete picture. This paper’s insight that human-generated tasks align better could suggest focusing on those for end-goal evaluation, using synthetic tasks more for diagnostic unit tests. Additionally, the finding about lexical items means we should control for or vary content to ensure a model isn’t just failing due to one odd word or symbol.
**Follow-up:** The paper essentially calls for establishing **more rigorous standards for benchmark validity**. One actionable takeaway is to incorporate *concurrence checks*: when we develop a new model, we should check if improvement on one benchmark also yields improvement on others (if not, investigate why). The field may move toward creating composite benchmark suites (similar to GLUE or BIG-Bench in NLP) for compositional generalization to address exactly this issue.

### “Most OOD Tests Reflect Interpolation, Not True Extrapolation” (Li et al., 2025)

**Authors:** Kangming Li, Andre N. Rubungo, Xiangyun Lei, Daniel Persaud, Kamal Choudhary, Brian DeCost, Adji B. Dieng, Jason Hattrick-Simpers
**Venue & Date:** *Communications Materials (Nature)*, 11 Jan 2025
**Relevance:** Although in the context of materials science, this study provides a cautionary evaluation insight applicable broadly: many supposed “OOD” evaluations are in fact not testing true extrapolation. The authors show that in their domain (predicting material properties), test sets thought to be distribution-shifted were actually within the convex hull of training data in representation space. This led to an overestimation of model generalizability and scaling benefits. This resonates strongly with our “evaluation illusion” – standard splits might be giving models too much credit. It emphasizes the need to rigorously ensure test cases are genuinely novel and challenging.
**Key Findings:**
* In evaluations of ML models on materials datasets intended to test extrapolation (like predicting properties of chemical compositions not seen in training), they found **most test samples were still in well-covered regions of feature space**. In other words, the splits were not as “out-of-distribution” as assumed.
* Consequently, even simple models (e.g., gradient-boosted trees) did well, and large neural networks showed improvements – but this was misleading because the tests were largely interpolation. When they isolated truly extrapolative cases (where test points lay outside training distribution), performance dropped sharply and scaling the model (bigger networks, more data) did *not* help much.
* They highlight that the **perception of neural scaling success** in these tasks was partly an artifact: models were being tested on easier interpolation problems, so adding data or parameters yielded gains, but on real extrapolation, those gains vanished or even reversed (more complex models overfit the wrong patterns).
* The authors call for **more physically meaningful OOD splits** – for example, splits based on different underlying causal factors, not just random or naive heuristics. They also suggest the community should be wary of declaring a method “generalizable” without thoroughly checking if test scenarios truly cover new ground.

⠀**Implications:** This directly informs how we should design and assess our benchmarks. It’s possible that some of our prior tests (like SCAN or CIFAR variants) might also allow models to interpolate in hidden ways. We need to ensure that when we test for compositional generalization, the held-out combinations are *truly novel*, not reachable by some continuous interpolation from seen ones. This might involve analyzing representation space coverage or even using domain knowledge to craft challenges (as Chollet did with ARC). Another practical point is that simply increasing model size or data is unlikely to yield extrapolation – instead, we need qualitatively new methods (architectures or training regimes). This supports our pursuit of architectural innovations rather than relying on scale.
**Action:** We should adopt the paper’s advice to create **rigorously challenging OOD benchmarks**. For example, we could incorporate tests analogous to their approach: measure similarity of each test point to training data (perhaps via embedding distance) to verify it’s an outlier. We might also consider *stress-testing models on purposefully extreme cases* and measuring at what point they break. The broader message – don’t conflate interpolation successes with true generalization – will guide us to be more critical of evaluation results.

### ARC-AGI-2: A New Challenge for Frontier AI Reasoning Systems (Chollet et al., 2025)

**Authors:** François Chollet, Mike Knoop, Gregory Kamradt, Bryan Landers, Henry Pinkard
**Venue & Date:** arXiv preprint (to be linked with ARC Challenge), May 2025
**Relevance:** Chollet’s *Abstraction and Reasoning Corpus (ARC)* has been a benchmark explicitly targeting fluid, human-like problem solving. ARC-AGI-2 is a **major update (2025)** to this benchmark, incorporating more tasks and finer-grained evaluation of abstract reasoning. This is highly relevant as an evaluation methodology advance: ARC-AGI-2 is designed to be *even harder to shortcut*, emphasizing true compositional invention and reasoning under minimal priors. Given our interest in distribution invention, ARC-AGI-2 represents exactly the kind of test where conventional ML has struggled (~34% is SOTA on the original ARC ) and new ideas are needed.
**Key Points:**
* **Preserves format, increases complexity:** ARC-AGI-2 keeps the same input-output grid puzzle format as ARC-1 (to maintain continuity for researchers), but introduces a *new curated set of tasks* that require higher-level reasoning and more steps of deduction. It aims to provide a more granular signal of capability – meaning it might categorize tasks by type or difficulty to see where AI systems fail.
* **Human baseline and cognitive focus:** The authors conducted **extensive human testing** on the new tasks to calibrate their difficulty. Humans can solve them (with effort), but current AI cannot, which underlines the gap. They emphasize *fluid intelligence* aspects: the ability to adapt and invent solutions to novel problems, not just recognize patterns.
* **Addresses ARC-1 limitations:** ARC-AGI-2’s design explicitly tries to fix issues identified in ARC-1. For instance, it likely reduces the dependency on pixel-level tricks or brute-force program synthesis (which some top ARC-1 submissions used) and instead includes tasks that require multi-step reasoning or generalization of a concept to a new context. The paper even discusses how ARC-1 competitions saw slow progress – leading to ARC-AGI-2 to push the envelope further. Notably, prize incentives have increased (over $1M for ARC 2024) to spur breakthroughs.
* **Design for compositional generalization:** The new tasks are *explicitly designed to require compositional skills* – e.g., combining two transformations, applying operations in sequence, or inventing a new operation from simpler ones. The authors provide examples in the paper where tasks demand applying known operations in novel combinations to succeed. This directly tests the kind of distribution **invention** we care about.

⠀**Implications:** ARC-AGI-2 stands as perhaps the **ultimate benchmark** for our research goal. Success on ARC-2 would be a strong indicator of a system’s compositional generalization and adaptability. Therefore, we should consider using ARC-AGI-2 tasks as a validation for any approach we develop. In terms of methodology, ARC-AGI-2’s introduction also suggests that the field is moving toward *more holistic evaluations* – not just single tasks but broad suites that capture different reasoning aspects. We might glean from ARC-AGI-2 the kinds of capabilities needed (the paper mentions things like object persistence, elementary geometry, etc., as core knowledge assumed, but no specific domain knowledge beyond that). This can guide what priors to include in models.
**Current State:** As of mid-2025, no AI model has significantly cracked ARC-AGI-1 (top models ~34% accuracy), and ARC-AGI-2 is even harder. This underscores the magnitude of the challenge – it likely requires some fundamental innovation (neuro-symbolic reasoning, causal induction, etc., all themes we are exploring). The ARC-AGI-2 announcement itself, with a competition and a prize, may drive a wave of research that we should watch and possibly participate in. It’s both an evaluation and an inspiration for the kinds of problems we should tackle.

### 5. Advances in Physics-Informed Neural Networks (PINNs) and Handling Distribution Shifts

Physics-Informed Neural Networks (PINNs) integrate differential equation constraints into neural models, aimed at leveraging physical laws for better generalization. A pertinent question for our research is how PINNs handle distribution shifts – e.g., changes in parameters, boundary conditions, or even the form of governing equations (mechanism changes). Recent developments (2023–2025) in this area include new PINN architectures that improve generalization across different discretizations or domains, techniques for time-varying parameters, and hybrid models maintaining physical conservation laws. We highlight a few key advances that enhance PINNs’ robustness to shifts, as these might inform how to enforce constraints in our models facing novel situations.

### Physics-Informed Discretization-Independent Deep Compositional Operator Network (PI-DCON) (Zhong & Meidani, 2024)

**Authors:** Weiheng Zhong, Hadi Meidani
**Venue & Date:** *Computer Methods in Applied Mechanics and Engineering*, Nov 2024 (also arXiv Apr 2024)
**Relevance:** Introduces a PINN-based architecture specifically designed to **generalize across different discretizations and resolutions** of PDE parameters. One traditional issue is that a PINN trained on one spatial grid or parameterization may not generalize to a finer grid or a different meshing of the domain. This PI-DCON model addresses that by learning an operator in a compositional form that is independent of the discretization. It directly tackles distribution shift in the form of *grid/domain changes*, aligning with our interest in models that adapt to structural changes.
**Key Contributions:**
* Proposes a **Deep Operator Network** that learns to map between function spaces (e.g., from a PDE coefficient function to the solution function) rather than just producing a solution for a fixed grid. By doing so, the network can be applied to inputs of varying resolution or discretization without retraining.
* Uses a **compositional formulation**: the operator is learned as a composition of simpler learned operators, which helps capture multi-scale behavior and improves generalization. For instance, they might break a complex operator into smaller sub-operators that handle local behavior, then compose them – a structurally compositional approach in the PINN context.
* Shows that the model **generalizes to various discrete representations** of PDE parameters. In experiments, after training on a certain mesh, PI-DCON was able to predict solutions on a much finer mesh or a differently structured mesh with minimal loss in accuracy, whereas a baseline PINN struggled. This indicates a robustness to distribution shift in the input representation.

⠀**Technical Approach:** The model likely builds on the concept of **neural operators** (like the Fourier Neural Operator, etc.) but with physics-informed training. It encodes PDE constraints and boundary conditions into the loss (making it “physics-informed”), while the architecture learns a mapping from input fields to output fields. The “discretization-independent” aspect might be achieved through using basis functions or a unified representation of functions (like Fourier features or graph-based representation of meshes) so that inputs on different grids can be handled uniformly. Compositionality in this context might refer to splitting the domain or operator (perhaps via a hierarchical basis or domain decomposition) and learning local operators that compose to the global solution.
**Results & Evaluation:** According to the abstract and references, PI-DCON achieved **high accuracy across different discretizations of the same problem**. For example, it might have been trained on solving a PDE with a coarse parameter field and then tested on a refined parameter field (more grid points for the same underlying function) – and it maintained accuracy, whereas a conventional network would require retraining or suffer in performance. They also mention generalization to various “PDE parameter representations”, implying different ways of encoding the input (like different basis expansions). The model’s performance on these varied inputs was nearly as good as on the original training representation, validating the approach. This is a significant result for engineering applications, as it means one can train a model once and use it in many scenarios (saving on retraining costs).
**Implications:** For our work, PI-DCON exemplifies how **architectural design (compositional operators) + physics constraints** can yield a model resilient to distribution shift. It’s an encouraging parallel: where they handle changes in discretization, we can analogously think of handling changes in task structure or environment. The key idea is learning in a way that abstracts from specifics – here abstracting from the grid. In general, it suggests that building **operator-level generalization** (learning mappings that hold in infinite-dimensional spaces, not just specific instances) is a powerful strategy. We might translate that idea to learning more abstract algorithms or rules that generalize beyond specific inputs. Moreover, this work reinforces the benefit of respecting known structure (physics laws in their case): by encoding such priors, the network’s predictions remain stable under shifts that respect those laws.
**Code/Data:** *Availability:* The authors have an official implementation on GitHub. This indicates the community can experiment with PI-DCON. If we consider physical domains in our research (like a PINN for a mechanism that might change), this code could be a foundation. For example, if we study time-varying physics (interest below), perhaps combining PI-DCON’s approach with time-varying parameter identification would be fruitful.

### Time-Varying Parameters and Mechanism Changes in PINNs

Recent PINN studies have extended the framework to **identify time-varying parameters or even fractional operators** in dynamical systems, which is directly relevant to handling mechanism changes. For instance, one 2024 study (Mondorf *et al*., 2024, referenced in [44]) applied PINNs to epidemiological models and successfully **recovered time-dependent transmission rates** and fractional orders of derivatives from data. By doing so, the PINN could adapt to distribution shifts where the governing equation’s parameters evolve (e.g., virus spread rate changing over time due to interventions). The results showed that a transformer-based PINN with meta-learning could systematically generalize to scenarios of combined transformations (translation+rotation in spatial tasks, in that specific case). The general take-away is that **meta-learning can enhance PINNs**: the model effectively “learned how to learn” the new parameters quickly, pointing to a direction where PINNs are not static but can adapt to structural changes (like an additional force appearing, or a conservation law being broken and then restored).

Furthermore, specialized PINN architectures enforce **conservation laws or symmetries** to guarantee physically plausible extrapolations. For example, *Divergence-free PINNs* ensure that a learned fluid flow remains incompressible by construction, and *Symplectic networks (SympNets)* conserve energy exactly in identified Hamiltonian systems. These innovations mean that even if the distribution shifts (say to a new region of state space), the network doesn’t violate fundamental physics like conservation of mass or energy, thereby maintaining validity. This is akin to providing strong inductive biases that guide generalization when data alone are insufficient.

**Implications:** For distribution shifts in physical problems (e.g., a mechanism change from laminar to turbulent flow, or a parameter shift from one material property to another), combining these strategies is promising. A model could detect a mechanism change via energy-based meta-learning (like EBML) and adjust its structure (like switching to a different set of equations or activating a different PINN module). In essence, the field is moving towards **modular physics-informed networks** – different modules handle different regimes (perhaps meta-learned which to apply) and constraints ensure continuity and feasibility.

In summary, the PINN advancements teach us that embedding known structure (PDEs, conservation laws), designing architecture to be flexible (discretization-independent, operator learning), and even using meta-learning for quick adaptation are all effective ways to handle distribution shift in scientific domains. These ideas are transferable: for any domain, incorporate the invariances and structures you know (be it grammar in language or causal laws in reasoning) and give your model the capacity to adapt structurally.

### 6. Meta-Learning Approaches for Structural Adaptation

Meta-learning has emerged as a tool not just for tuning parameters quickly to new tasks, but also for enabling models to adapt their *structure or strategy* when facing novel scenarios. The question is: can meta-learning help a model *discover new primitives or architectures* needed for extrapolation, rather than just fine-tuning weights? Recent literature (2023–2025) provides some clues. We have meta-learning methods that achieved human-like systematic generalization by dynamically adjusting training distributions (as in MLC), and others that introduce new architectures (like CAMEL) where meta-learning yields interpretable parameters tied to physical meaning. Below, we review notable works in this vein, focusing on how they address adaptation beyond simple parameter tweaking.

### Human-like Systematic Generalization through Meta-Learning (MLC) (Lake & Baroni, 2023)

**Authors:** Brenden M. Lake, Marco Baroni
**Venue & Date:** *Nature*, 25 Oct 2023
**Relevance:** This groundbreaking work demonstrated a standard neural network achieving **human-level systematic generalization** by means of a meta-learning training paradigm  . While the base architecture wasn’t new (they used a standard sequence-to-sequence model), the *training procedure (Meta-Learning for Compositionality, MLC)* effectively altered how the model learns, guiding it to develop a more structural form of generalization. It falls under theme 6 because meta-learning here isn’t just learning a few initial weights – it’s *shaping the model’s ability to learn new concepts compositionally*, arguably a form of structural adaptation in its skillset.
**Key Contributions:**
* Introduces the **MLC training regime**, where the model is trained on a *dynamic curriculum* of tasks drawn from a compositional task distribution . The tasks are generated in episodes such that the model is forced to continually learn new combinations of concepts (like few-shot learning of new words in an instruction learning setup). This meta-training mimics human learning situations and optimizes the model’s *learning algorithm* for compositional skills, not just performance on one fixed task  .
* Conducts **human experiments in parallel** and directly compares machine and human performance on the same few-shot instruction tasks . The finding: MLC-trained models not only far outperformed conventionally trained models on systematic generalization, but they also matched human behavior patterns in key ways (achieving both systematicity and flexibility, whereas symbolic models are systematic but inflexible, and neural nets vice versa) . This was a strong validation that the meta-learned training strategy instilled a human-like compositional inductive bias.
* Showed **improvements on systematic generalization benchmarks**: MLC advanced state-of-the-art on several benchmarks and tasks (the paper mentions systematic generalization benchmarks beyond their custom instruction task) . It effectively solved tasks that defeated standard training, such as certain splits of SCAN or generalization in artificial languages, indicating the learned learning-to-learn approach was broadly beneficial.

⠀**Technical Approach:** In each meta-learning episode, the model receives a small set of examples (e.g., new command → action demonstrations) and must produce correct outputs for new queries after that small “training”. The MLC optimizer (outer loop) updates the model parameters so that it can quickly adapt in these episodes. Crucially, the distribution of episodes is structured to emphasize *combinatorial novelty* – e.g., introducing a new word and testing it in combination with known words (like learning “jump” then understanding “jump around left twice”)  . Over many such episodes, the model essentially learns an *inductive bias for compositionality*: it internalizes that new concepts should be combined systematically with old ones. This is meta-learning not of a static task, but of *how to learn any new concept in a compositional way*.
**Results & Evaluation:** MLC-trained models achieved near-perfect generalization in scenarios where regular training yielded almost zero (for instance, certain SCAN splits where a new verb appears only in simple contexts at training time but must be understood in complex contexts at test time) . In the human vs model comparisons, only the MLC model mirrored humans in being able to one-shot learn a new word and use it in novel contexts immediately . The rigid symbolic baseline succeeded systematically but lacked flexibility (couldn’t handle ambiguity or exceptions), while a vanilla neural net was flexible but not systematic. MLC strikingly achieved both . This result was heralded as addressing the decades-old Fodor & Pylyshyn challenge to neural networks.
**Implications:** MLC provides a template for **meta-learning as a way to instill structural priors**. Instead of manually engineering an architecture with a compositional prior, they “learned” that prior through a clever training regime. For our aims, this suggests we can use meta-learning to simulate scenarios that force a model to become more adaptable. It also suggests a route to handle *structural changes*: one could meta-train a model on a variety of environment dynamics or rules, so that encountering a new rule at test time isn’t a shock – the model has learned how to learn new rules. Indeed, one could conceive of an MLC variant for physical systems: train a model across simulations with varying laws, so it learns the concept of adapting to a new physics. MLC underscores that *the learning process itself can be trained*, which is a powerful idea for extrapolation.
**Code/Data:** The authors did not publicly release code at publication, but the concept is clearly described. Since then, others (like Mondorf *et al.* 2025 in theme 1) have built on MLC, and a few open-source reimplementations exist (the GitHub mainlp/SYGAR is one example, extending MLC to spatial reasoning). So in practice, one can experiment with MLC-like curricula.

*(We note: as discussed in Theme 3, a follow-up position paper argued that MLC, while a breakthrough, might not fully solve compositionality in all cases. This indicates that combining MLC with architectural innovations might be the path forward.)*

### Enabling Systematic Generalization in Abstract Spatial Reasoning through Meta-Learning (Mondorf et al., 2025)

*(This work extends Lake & Baroni’s MLC to a new domain and is an example of meta-learning yielding structural adaptability beyond language.)*
**Authors:** Philipp Mondorf, Shijia Zhou, Monica Riedler, Barbara Plank
**Venue & Date:** arXiv preprint, April 2025 (under review)
**Relevance:** This paper applied the MLC meta-learning approach to a very different domain: **visual geometric transformations**. It introduced a dataset “SYGAR” for systematic generalization in 2D abstract scenes (transforming shapes). The significance is that it shows meta-learning for compositionality is not limited to language – it can adapt a model to *structural changes in tasks involving spatial reasoning* as well. Additionally, they compared their meta-learned transformer with state-of-the-art large models (GPT-4 variants) and found the meta-learned model systematically generalizes while the large models do not. This underscores meta-learning’s role in structural adaptation: the model effectively learned how to handle combinations of transformations never seen jointly before.
**Key Contributions:**
* Created **SYGAR (Systematic Generalization in AR)**, a benchmark where models see certain primitive transformations (e.g., rotate, reflect, color change) in isolation or simple combinations during training, and must generalize to novel combinations at test (like rotate+reflect together, etc.). This parallels SCAN but in a visual domain.
* Showed that a **transformer encoder-decoder meta-trained via MLC** on SYGAR can **systematically generalize to unseen transformation compositions with high accuracy**, whereas powerful pretrained models (including a finetuned GPT-4 vision model and others) fail to do so. The meta-learned model significantly outperforms these baselines, indicating that the compositional skill didn’t emerge from scale alone but did emerge from the meta-training strategy.
* Reinforces the evidence that **meta-learning for compositionality (MLC)** is a general principle: it improved systematic generalization *beyond language* – suggesting it’s capturing something fundamental about learning to compose concepts, not just exploiting linguistic structure.

⠀**Results & Evaluation:** The meta-trained model achieved near perfect scores on tasks like applying two or three transformations in combination, even if some of those combinations were never in the training data. For example, perhaps the model saw “rotate 90°” alone and “change color to red” alone in training, and at test time it correctly handles an instruction to “rotate 90° *and* change color to red” on a shape it hadn’t seen that combination for. The baseline large models, despite their capacity, often output incorrect results for novel combos (like they might do one transform but not the other, or both in wrong order). This highlights that *targeted meta-learning beat general-purpose learning in this extrapolation scenario*.
**Implications:** For our research, Mondorf et al. show that **meta-learning can be domain-agnostic** in instilling compositional priors. It encourages us to consider meta-learning whenever we have a family of tasks with underlying compositional rules. If structural adaptation is needed (e.g., new mechanics in a physics simulation), a meta-trained model might adapt to those better than a static model. It also validates using smaller specialized models with meta-training as an alternative to solely relying on massive pre-trained models for OOD generalization; the latter can fail if not explicitly trained for systematicity. The fact that GPT-4 failed tests that a small meta-trained model passed is telling: *experience in the right training regime can trump sheer size*. This is an important strategic point for us given limited resources – *how* we train might matter more than *how big* the model is, for these kinds of problems.
**Code/Data:** Yes, as noted earlier, the authors released their code and data on GitHub (SYGAR code repository). This means we could directly use SYGAR as a testbed and possibly adapt their meta-learning code to other domains (it’s likely flexible to different tasks given the nature of the approach).

### Interpretable Meta-Learning of Physical Systems (CAMEL) (Blanke & Lelarge, 2024)

**Authors:** Matthieu Blanke, Marc Lelarge
**Venue & Date:** *ICLR 2024* (poster), Jan 2024
**Relevance:** CAMEL is a meta-learning architecture that is specifically designed for physical systems and promises **interpretable and efficient meta-learning across multiple environments**. It addresses structural adaptation by assuming the functional form of the model has an *affine structure w.r.t. tasks*, which allows it to identify physical parameters for each environment. Essentially, it’s a meta-learning model that yields not just good performance but also human-understandable parameters (e.g., mass, friction coefficients) that differ per task/environment. This directly tackles architectural adaptation: the architecture has a special structure to facilitate learning new tasks with minimal cost and extracting explicit differences.
**Key Contributions:**
* Introduces a new architecture (CAMEL) for meta-learning that has **built-in interpretability** by design. Because of an affine decomposition, when it learns from multiple environments, it can separate what is common structure and what are environment-specific parameters.
* Proves theoretical identifiability: under certain conditions, CAMEL can correctly learn the true physical parameters of each environment from data. This means if, say, we have a pendulum with different lengths in different environments, CAMEL could meta-learn the equation form (common) and each environment’s length (specific), recovering ground truth.
* Shows strong performance on a variety of physical scenarios (from toy analytical physics to complex robotics or control systems) with far lower computational cost than black-box meta-learning like recurrent models or neural processes. Additionally, the learned parameters make sense (e.g., matching known values) demonstrating interpretability.

⠀**Technical Approach:** CAMEL likely assumes the model’s prediction for an environment can be expressed as $f_\theta(x) + \alpha_e \cdot g_\theta(x)$ (simplified example of affine form), where $\alpha_e$ is a vector of environment-specific coefficients and $f, g$ are shared neural computations. Meta-learning then involves learning $\theta$ (shared) and $\alpha_e$ for each environment. This structure means learning new environments is as simple as learning a new set of coefficients (linear, thus data-efficient) while reusing the bulk of the model. It’s akin to a linear basis for tasks, which yields interpretability (coefficients might correspond to physical constants). They apply it in contexts like electrostatics or control, where, say, an environment might differ by a physical constant. Standard meta-learning would treat it all as black box, but CAMEL’s structure allows it to pinpoint “environment 5 has parameter X=2.5” which is meaningful.
**Results & Evaluation:** The paper reports that on tasks like identifying parameters of differential equations, CAMEL achieved competitive generalization with orders of magnitude fewer training iterations, and it correctly inferred the hidden parameters. In an adaptive control scenario, CAMEL could adapt to new systems very quickly and the adaptation is interpretable (like it knows how much to adjust a gain for a new robot). Compared to MAML or others, CAMEL was faster and its solutions were understandable (which is crucial in physical applications for safety and insight).
**Implications:** CAMEL highlights that **imposing a bit of structure in meta-learning can yield big benefits**. For our interests, if we suspect tasks have a common form with just a few varying factors, an approach like CAMEL’s could be ideal. It’s a form of *meta-learning beyond brute force*: using an architecture that expects an underlying parametric family. This resonates with our earlier themes: identify what to keep invariant and what to adapt. In a way, CAMEL is performing a simple form of causal representation learning at the task level (common causal law with different parameters per context). We might draw inspiration from this: e.g., could we design a meta-learner for compositional tasks that assumes each new task is a composition of known primitives with different weights? That might yield interpretability (like “in this novel task, rule A and rule B are combined with weight 0.7/0.3”). CAMEL’s success indicates that pushing for interpretability doesn’t necessarily cost performance – often it improves data efficiency, which is a bonus.
**Code/Data:** Not sure about an official release. Given it’s a theoretical and applied work, the authors might share their implementation upon request or later. The tasks they used vary, but many are standard physics or control problems (some simulation code likely custom). If we need to implement similar ideas, the paper provides the conceptual blueprint.

⸻

**Concluding Remarks:** Across these themes, the literature from 2023–2025 paints a picture of *rapid progress coupled with deeper understanding*. We saw that **fundamental capabilities** like variable binding are being achieved in networks (sometimes implicitly), yet explicit mechanisms and careful evaluation are still crucial. **Architectural innovations** – modular designs, causal learners, energy-based frameworks – are actively being developed to tackle OOD generalization from different angles. **Neuro-symbolic hybrids** have reached new heights, effectively solving tasks once thought intractable for neural nets, by embedding symbolic reasoning. At the same time, researchers are critically assessing **evaluation methods**, ensuring we are not fooling ourselves with easy test splits and instead raising the bar with challenges like ARC-AGI-2. In domains like physics, **PINNs** are becoming more robust by integrating domain knowledge and meta-learning, suggesting a template for any domain: incorporate invariants and adapt to changes. Finally, **meta-learning** has proven to be a powerful paradigm for instilling inductive biases and rapid adaptation capabilities, sometimes yielding paradigm-shifting results (as with MLC’s human-level generalization).

For our work on “rethinking distribution invention,” these insights converge on a few key takeaways:
* **Incorporate Structure**: Whether via neuro-symbolic modules, causal representations, or invariants (physics or otherwise), giving the model some structure to latch onto dramatically improves true generalization. Pure black-box approaches are increasingly outperformed by those with structured components.
* **Train for Generalization**: Techniques like MLC show that *how* we train (curricula, meta-learning tasks) can make a neural network generalize far beyond its normal limits. Designing training regimes that explicitly test a model’s compositional and extrapolative abilities (and possibly meta-training it to do well on them) is as important as model architecture.
* **Evaluate Rigorously**: We must evaluate models on a battery of tests (different benchmarks, adversarial cases, higher-order combinations) to ensure we’re seeing the real picture. Behavioral testing and new benchmarks like ARC-AGI-2 will be essential to verify if a proposed solution truly scales to “inventing” new distributions.
* **Interdisciplinary Insights**: Ideas from cognitive science (human experiments, developmental patterns), from logic (symbolic regimens), and from physics (PINNs, conservation laws) are all feeding into machine learning advances. Our research should remain similarly interdisciplinary, not hesitating to borrow techniques or metaphors from these fields.

⠀
In summary, the period 2023–2025 has been rich in both **breakthroughs** (like meta-learned compositional skills, state-of-the-art neuro-symbolic systems) and **sobering analyses** (on the limits of current models and evaluations). These will inform the next steps of our project, as we design models that not only fit the data but *fundamentally understand* and *generalize* in human-like ways. We now have an arsenal of new methods and a clearer vision of pitfalls to avoid, which together will guide us in building and testing the “different approach” that our earlier failures have prompted us to seek.
