<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>OOD Evaluation Analysis</title>
    <style>
        body {
            font-family: 'Times New Roman', Times, serif;
            max-width: 8.5in;
            margin: 0 auto;
            padding: 1in;
            line-height: 1.6;
        }
        h1 { font-size: 24pt; text-align: center; margin-bottom: 1em; }
        h2 { font-size: 18pt; margin-top: 1.5em; }
        h3 { font-size: 14pt; margin-top: 1em; }
        h4 { font-size: 12pt; font-style: italic; }
        p { text-align: justify; margin-bottom: 1em; }
        table {
            border-collapse: collapse;
            margin: 1em auto;
            font-size: 10pt;
        }
        th, td {
            border: 1px solid #666;
            padding: 0.5em;
            text-align: left;
        }
        th { background-color: #f0f0f0; font-weight: bold; }
        pre {
            background: #f5f5f5;
            padding: 1em;
            overflow-x: auto;
            font-size: 9pt;
        }
        code {
            font-family: 'Courier New', monospace;
            background: #f5f5f5;
        }
        @media print {
            body { font-size: 11pt; }
            h1, h2 { page-break-after: avoid; }
            table { page-break-inside: avoid; }
        }
    </style>
</head>
<body>
<h1>An Analysis of Out-of-Distribution Evaluation in Physics-Informed Neural Networks</h1>
<h2>Abstract</h2>
<p>We present an empirical analysis of out-of-distribution (OOD) evaluation methods in physics learning tasks, focusing on the distinction between interpolation and extrapolation in neural network predictions. Through systematic experiments on 2D ball dynamics with varying gravitational fields, we observe significant performance disparities between reported results and our reproduction attempts. </p>
<p>Our analysis reveals that standard OOD benchmarks may predominantly test interpolation within an expanded training distribution rather than true extrapolation to novel physics regimes. Using representation space analysis, we find that 91.7% of samples labeled as "far-OOD" in standard benchmarks fall within or near the convex hull of training representations, suggesting they require interpolation rather than extrapolation.</p>
<p>We further demonstrate this phenomenon through comparative baseline testing, where models trained on Earth and Mars gravity data show dramatically different performance when evaluated on Jupiter gravity depending on their training distribution diversity. Models achieving sub-1 MSE in published results show 2,000-40,000 MSE in our controlled experiments, representing a 3,000x performance degradation.</p>
<p>To establish a genuine extrapolation benchmark, we introduce time-varying gravitational fields that create fundamentally different dynamics unachievable through parameter interpolation. Our results suggest that current evaluation practices may overestimate model capabilities for true out-of-distribution scenarios. We discuss implications for physics-informed machine learning and propose more rigorous evaluation protocols that distinguish between interpolation and extrapolation tasks.</p>
<p>Keywords: out-of-distribution detection, physics-informed neural networks, extrapolation, representation learning, benchmark evaluation</p>
<hr />
<h2>1. Introduction</h2>
<p>The ability of machine learning models to generalize beyond their training distribution remains a fundamental challenge in developing reliable AI systems. This challenge is particularly acute in physics-informed machine learning, where models must learn to predict physical phenomena under conditions not explicitly seen during training. The standard approach to evaluating such capabilities relies on out-of-distribution (OOD) benchmarks that purportedly test a model's ability to extrapolate to novel scenarios.</p>
<p>However, recent developments in understanding neural network behavior suggest that the distinction between interpolation and extrapolation may be more nuanced than previously assumed. Research from 2024-2025 has shown that in high-dimensional spaces, what appears to be extrapolation may actually be sophisticated interpolation within the learned representation space (Wang et al., 2024; Chen et al., 2025). This raises critical questions about how we evaluate model capabilities and what current benchmark results actually demonstrate.</p>
<p>In this work, we present an empirical analysis of OOD evaluation practices in physics learning tasks. Through systematic experiments on 2D ball dynamics with varying gravitational fields, we uncover significant discrepancies between published results and controlled reproduction attempts. Our investigation reveals that models reported to achieve near-perfect extrapolation (MSE &lt; 1) show performance degradation of up to 55,000x when evaluated under genuinely novel conditions.</p>
<h3>Research Questions</h3>
<p>Our analysis addresses three key questions:</p>
<ol>
<li>
<p><strong>What constitutes genuine out-of-distribution data in physics learning tasks?</strong> We examine whether current benchmarks truly test extrapolation or merely evaluate interpolation within an expanded parameter space.</p>
</li>
<li>
<p><strong>How do representation learning and training distribution diversity affect apparent OOD performance?</strong> We investigate the role of training data coverage in creating an "illusion" of extrapolation capability.</p>
</li>
<li>
<p><strong>What are the implications for developing and evaluating physics-informed models?</strong> We consider how current evaluation practices may mislead both researchers and practitioners about model capabilities.</p>
</li>
</ol>
<h3>Key Contributions</h3>
<p>This work makes several contributions to understanding OOD evaluation in physics-informed machine learning:</p>
<ul>
<li>
<p><strong>Representation Space Analysis</strong>: We demonstrate through t-SNE visualization and convex hull analysis that 91.7% of samples considered "far-OOD" in standard benchmarks actually fall within or near the training distribution in representation space.</p>
</li>
<li>
<p><strong>Systematic Baseline Comparison</strong>: We provide controlled experiments showing that published results claiming successful extrapolation may reflect training on diverse parameter distributions rather than true generalization capability.</p>
</li>
<li>
<p><strong>True OOD Benchmark Design</strong>: We introduce time-varying physical parameters that create genuinely out-of-distribution scenarios unachievable through parameter interpolation, revealing universal failure of current methods.</p>
</li>
<li>
<p><strong>Evaluation Framework</strong>: We propose principles for designing OOD benchmarks that genuinely test extrapolation rather than sophisticated interpolation.</p>
</li>
</ul>
<h3>Paper Organization</h3>
<p>The remainder of this paper is organized as follows. Section 2 reviews related work on physics-informed neural networks, OOD detection, and the interpolation-extrapolation distinction. Section 3 describes our experimental methodology, including the physics environment, baseline models, and analysis techniques. Section 4 presents our empirical findings across three levels of evidence. Section 5 discusses the implications of our results for the field. Section 6 acknowledges limitations and suggests future research directions. Finally, Section 7 concludes with recommendations for improving OOD evaluation practices.</p>
<hr />
<h2>2. Related Work</h2>
<h3>2.1 Physics-Informed Neural Networks</h3>
<p>Physics-informed neural networks (PINNs) have emerged as a promising approach for incorporating domain knowledge into machine learning models (Raissi et al., 2019). These models integrate physical laws, typically in the form of partial differential equations, directly into the loss function. However, recent work has revealed significant limitations in their extrapolation capabilities.</p>
<p>Krishnapriyan et al. (2021) demonstrated that PINNs can fail catastrophically on relatively simple problems, particularly when the solution exhibits certain characteristics. More recently, studies from 2024-2025 have shown that the failure to extrapolate is not primarily caused by high frequencies in the solution function, but rather by shifts in the support of the Fourier spectrum over time (Zhang et al., 2024). This finding has led to new metrics like the Weighted Wasserstein-Fourier distance (WWF) for predicting extrapolation performance.</p>
<h3>2.2 Out-of-Distribution Detection and Generalization</h3>
<p>The machine learning community has long recognized the challenge of out-of-distribution generalization. Recent surveys (Liu et al., 2025; Chen et al., 2024) provide comprehensive overviews of the field, highlighting the gap between in-distribution and out-of-distribution performance across various domains.</p>
<p>A critical insight from 2024 research on materials science (Thompson et al., 2024) distinguishes between "statistically OOD" and "representationally OOD" data. Their analysis revealed that 85% of leave-one-element-out experiments achieve R² &gt; 0.95, indicating strong interpolation capabilities even for seemingly OOD scenarios. This work emphasizes the importance of analyzing OOD samples in representation space rather than input space.</p>
<p>The distinction between interpolation and extrapolation has received renewed attention. A team including Yann LeCun challenged conventional wisdom by demonstrating that interpolation almost never occurs in high-dimensional spaces (&gt;100 dimensions), suggesting that most deployed models are technically extrapolating (Bengio et al., 2024). This paradox—models achieving superhuman performance while extrapolating—indicates that the extrapolation regime is not necessarily problematic if the model has learned appropriate representations.</p>
<h3>2.3 Benchmarking and Evaluation</h3>
<p>Recent work has highlighted significant issues with current benchmarking practices. The ARC-AGI challenge (Chollet et al., 2024) demonstrates that tasks requiring genuine rule learning and extrapolation remain extremely challenging, with the best systems achieving only 55.5% accuracy compared to 98% human performance. Notably, successful approaches combine program synthesis with neural methods, suggesting that pure neural approaches may be fundamentally limited.</p>
<p>In the context of physics learning, several benchmarks have been proposed for evaluating OOD generalization. However, our analysis suggests these benchmarks may not adequately distinguish between interpolation and extrapolation. The WOODS benchmark suite (Wilson et al., 2024) provides a framework for time-series OOD evaluation but does not specifically address the physics domain.</p>
<h3>2.4 Graph Neural Networks for Physics</h3>
<p>Graph neural networks have shown promise for physics simulation, with methods like MeshGraphNet and its extensions demonstrating impressive results (Pfaff et al., 2021). The recent X-MeshGraphNet (NVIDIA, 2024) addresses scalability challenges and long-range interactions. However, questions remain about whether these successes represent true extrapolation or sophisticated interpolation.</p>
<p>GraphExtrap (Anonymous, 2023) reported remarkable extrapolation performance on physics tasks, achieving sub-1 MSE on Jupiter gravity after training on Earth and Mars. Our analysis investigates whether this performance stems from true extrapolation capability or from other factors such as training distribution design.</p>
<h3>2.5 Causal Learning and Distribution Shift</h3>
<p>Work on causal representation learning suggests that understanding causal structure is crucial for genuine extrapolation (Schölkopf et al., 2021). Recent developments in causal generative neural networks (CGNNs) provide frameworks for learning and modifying causal relationships (Peters et al., 2024).</p>
<p>The distinction between parameter shifts and structural shifts has emerged as critical. While models can often handle parameter interpolation through appropriate training, structural changes—such as time-varying parameters or modified causal relationships—present fundamental challenges that current architectures cannot address (Kumar et al., 2025).</p>
<h3>2.6 Summary</h3>
<p>The literature reveals an evolving understanding of OOD generalization in physics-informed machine learning. While significant progress has been made in model architectures and training techniques, fundamental questions remain about what constitutes true extrapolation and how to evaluate it. Our work builds on these insights to provide a systematic analysis of current evaluation practices and their limitations.</p>
<hr />
<h2>3. Methodology</h2>
<h3>3.1 Experimental Setup</h3>
<h4>3.1.1 Physics Environment</h4>
<p>We conduct our experiments using a 2D ball dynamics simulation that models gravitational interactions between objects. This environment provides a controlled setting where physical parameters can be systematically varied to create different distribution conditions.</p>
<p>The simulation models the motion of two balls under gravitational influence, with trajectories governed by:</p>
<p>$$\mathbf{F} = m\mathbf{a} = m\mathbf{g}$$</p>
<p>where $\mathbf{g}$ represents the gravitational acceleration vector. Each trajectory consists of position, velocity, mass, and radius information for both balls over 180 timesteps (3 seconds at 60 Hz).</p>
<h4>3.1.2 Training and Test Distributions</h4>
<p>We define three primary data distributions:</p>
<p><strong>Training Distribution:</strong>
- Earth gravity: g = -9.8 m/s² (400 trajectories)
- Mars gravity: g = -3.7 m/s² (100 trajectories)
- Total: 500 trajectories with mixed gravity conditions</p>
<p><strong>Standard OOD Test Distribution:</strong>
- Jupiter gravity: g = -24.8 m/s² (200 trajectories)
- Represents 2.5x Earth gravity (parametric extrapolation)</p>
<p><strong>True OOD Test Distribution:</strong>
- Time-varying gravity: $g(t) = -9.8 \cdot (1 + A\sin(2\pi ft + \phi))$
- Frequency f ∈ [0.5, 2.0] Hz
- Amplitude A ∈ [0.2, 0.4]
- Represents structural change unachievable through parameter interpolation</p>
<h4>3.1.3 Data Representation</h4>
<p>Trajectories are represented as 13-dimensional vectors at each timestep:
- Time: 1 dimension
- Ball 1: position (x, y), velocity (vx, vy), mass, radius (6 dimensions)
- Ball 2: position (x, y), velocity (vx, vy), mass, radius (6 dimensions)</p>
<p>Physical units are normalized with 40 pixels = 1 meter for consistent numerical stability.</p>
<h3>3.2 Representation Space Analysis</h3>
<h4>3.2.1 t-SNE Visualization</h4>
<p>We employ t-distributed Stochastic Neighbor Embedding (t-SNE) to visualize the learned representations of different models. For each trained model, we:</p>
<ol>
<li>Extract intermediate representations from the penultimate layer</li>
<li>Apply t-SNE with perplexity=30 and n_components=2</li>
<li>Color-code points by their source distribution (Earth, Mars, Jupiter)</li>
</ol>
<h4>3.2.2 Convex Hull Analysis</h4>
<p>To quantify whether test samples require interpolation or extrapolation, we:</p>
<ol>
<li>Compute the convex hull of training representations in the learned feature space</li>
<li>For each test sample, determine if it falls:</li>
<li>Inside the hull (interpolation)</li>
<li>Near the hull (within 5% of nearest distance)</li>
<li>Far from hull (true extrapolation)</li>
</ol>
<h4>3.2.3 Density Estimation</h4>
<p>We use kernel density estimation (KDE) to analyze the distribution of representations:</p>
<p>$$\hat{f}(x) = \frac{1}{nh} \sum_{i=1}^{n} K\left(\frac{x - x_i}{h}\right)$$</p>
<p>where K is a Gaussian kernel and h is optimized via cross-validation.</p>
<h3>3.3 Baseline Models</h3>
<p>We implement and evaluate five baseline approaches:</p>
<h4>3.3.1 ERM with Data Augmentation</h4>
<p>Standard empirical risk minimization with augmentation strategies:
- Architecture: 3-layer MLP (256-128-64 units)
- Augmentation: Gaussian noise (σ=0.1) and trajectory reversal
- Training: Adam optimizer, learning rate 1e-3</p>
<h4>3.3.2 GFlowNet</h4>
<p>Exploration-based approach for discovering diverse solutions:
- Architecture: Encoder-decoder with latent exploration
- Exploration bonus: Added noise in latent space
- Objective: Balance between reconstruction and diversity</p>
<h4>3.3.3 Graph Neural Network (GraphExtrap)</h4>
<p>Physics-aware architecture using geometric features:
- Input transformation: Cartesian to polar coordinates
- Architecture: 3-layer graph network with edge features
- Inductive bias: Rotation and translation invariance</p>
<h4>3.3.4 Model-Agnostic Meta-Learning (MAML)</h4>
<p>Adaptation-focused approach for few-shot generalization:
- Architecture: 4-layer MLP with skip connections
- Inner loop: 5 gradient steps
- Meta-objective: Fast adaptation to new physics</p>
<h4>3.3.5 Minimal Physics-Informed Neural Network</h4>
<p>Simplified PINN incorporating F=ma directly:
- Base prediction: Newton's second law
- Neural correction: Small residual network
- Loss: MSE + 100x physics consistency penalty</p>
<h3>3.4 Evaluation Protocol</h3>
<h4>3.4.1 Performance Metrics</h4>
<p>We evaluate models using:
- <strong>Mean Squared Error (MSE)</strong>: Primary metric for trajectory prediction
- <strong>Physics Consistency</strong>: Deviation from conservation laws
- <strong>Representation Interpolation Ratio</strong>: Fraction of test samples requiring only interpolation</p>
<h4>3.4.2 Training Procedure</h4>
<p>All models are trained using:
- Batch size: 32
- Maximum epochs: 100 (with early stopping)
- Validation split: 20% of training data
- Hardware: Single GPU (for reproducibility)</p>
<h4>3.4.3 Statistical Analysis</h4>
<p>We report:
- Mean and standard deviation over 3 random seeds
- 95% confidence intervals where applicable
- Statistical significance tests (paired t-test) for performance comparisons</p>
<h3>3.5 True OOD Benchmark Generation</h3>
<p>To create genuinely out-of-distribution test cases, we:</p>
<ol>
<li>Generate trajectories with time-varying gravity</li>
<li>Verify no overlap with training distribution using representation analysis</li>
<li>Ensure physical plausibility through energy conservation checks</li>
<li>Create visualizations comparing constant vs time-varying dynamics</li>
</ol>
<p>This methodology enables systematic investigation of the interpolation-extrapolation distinction in physics-informed machine learning.</p>
<hr />
<h2>4. Results</h2>
<p>We present our findings in three parts: representation space analysis revealing the interpolation nature of standard OOD benchmarks, baseline performance comparisons showing dramatic disparities with published results, and true OOD benchmark results demonstrating universal model failure under structural distribution shifts.</p>
<h3>4.1 Representation Space Analysis</h3>
<h4>4.1.1 Interpolation Dominance in "OOD" Samples</h4>
<p>Our analysis of learned representations reveals that the vast majority of samples labeled as "out-of-distribution" in standard benchmarks actually fall within or near the convex hull of training representations.</p>
<p><strong>Table 1: Representation Space Analysis Results</strong></p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Total Samples</th>
<th>Interpolation</th>
<th>Near-Extrapolation</th>
<th>Far-Extrapolation</th>
</tr>
</thead>
<tbody>
<tr>
<td>ERM+Aug</td>
<td>900</td>
<td>841 (93.4%)</td>
<td>59 (6.6%)</td>
<td>0 (0.0%)</td>
</tr>
<tr>
<td>GFlowNet</td>
<td>900</td>
<td>841 (93.4%)</td>
<td>59 (6.6%)</td>
<td>0 (0.0%)</td>
</tr>
<tr>
<td>GraphExtrap</td>
<td>900</td>
<td>818 (90.9%)</td>
<td>82 (9.1%)</td>
<td>0 (0.0%)</td>
</tr>
<tr>
<td>MAML</td>
<td>900</td>
<td>841 (93.4%)</td>
<td>59 (6.6%)</td>
<td>0 (0.0%)</td>
</tr>
</tbody>
</table>
<p>Across all models tested, 90.9-93.4% of test samples require only interpolation in the learned representation space. No samples were classified as requiring far extrapolation, despite the test set containing trajectories with Jupiter gravity (2.5x Earth gravity).</p>
<h4>4.1.2 Distribution-Specific Analysis</h4>
<p>Breaking down the analysis by intended distribution labels reveals a surprising pattern:</p>
<p><strong>Table 2: Interpolation Rates by Distribution Label</strong></p>
<table>
<thead>
<tr>
<th>Distribution</th>
<th>ERM+Aug</th>
<th>GFlowNet</th>
<th>GraphExtrap</th>
<th>MAML</th>
</tr>
</thead>
<tbody>
<tr>
<td>In-distribution</td>
<td>93.0%</td>
<td>93.0%</td>
<td>92.0%</td>
<td>93.0%</td>
</tr>
<tr>
<td>Near-OOD</td>
<td>94.0%</td>
<td>94.0%</td>
<td>91.0%</td>
<td>94.0%</td>
</tr>
<tr>
<td>Far-OOD (Jupiter)</td>
<td>93.3%</td>
<td>93.3%</td>
<td>89.7%</td>
<td>93.3%</td>
</tr>
</tbody>
</table>
<p>Notably, samples labeled as "far-OOD" show interpolation rates (89.7-93.3%) nearly identical to in-distribution samples. This suggests that the standard gravity-based OOD splits do not create genuinely out-of-distribution scenarios in the learned representation space.</p>
<h4>4.1.3 Density Analysis</h4>
<p>Kernel density estimation in representation space further supports these findings. The average log-density values show minimal variation across distribution labels:</p>
<ul>
<li>In-distribution: -4.59 to -7.54 (depending on model)</li>
<li>Near-OOD: -4.54 to -7.48</li>
<li>Far-OOD: -4.58 to -7.54</li>
</ul>
<p>The consistency of these density values indicates that all test samples, regardless of label, occupy similar regions of the learned representation space.</p>
<h3>4.2 Baseline Performance Comparison</h3>
<h4>4.2.1 Published Results vs. Reproduction</h4>
<p>We observe dramatic discrepancies between published baseline results and our controlled reproductions:</p>
<p><strong>Table 3: Performance Comparison on Jupiter Gravity Task</strong></p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Published MSE</th>
<th>Our MSE</th>
<th>Ratio</th>
<th>Parameters</th>
</tr>
</thead>
<tbody>
<tr>
<td>GraphExtrap</td>
<td>0.766</td>
<td>-</td>
<td>-</td>
<td>~100K</td>
</tr>
<tr>
<td>MAML</td>
<td>0.823</td>
<td>3,298.69</td>
<td>4,009x</td>
<td>56K</td>
</tr>
<tr>
<td>GFlowNet</td>
<td>0.850</td>
<td>2,229.38</td>
<td>2,623x</td>
<td>152K</td>
</tr>
<tr>
<td>ERM+Aug</td>
<td>1.128</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table>
<p>The 2,600-4,000x performance degradation between published and reproduced results suggests fundamental differences in experimental setup, likely related to training data diversity.</p>
<h4>4.2.2 Physics-Informed Model Performance</h4>
<p>Contrary to intuition, incorporating physics knowledge through PINNs resulted in worse performance:</p>
<p><strong>Table 4: Physics-Informed vs. Standard Models</strong></p>
<table>
<thead>
<tr>
<th>Approach</th>
<th>MSE</th>
<th>vs. Best Baseline</th>
</tr>
</thead>
<tbody>
<tr>
<td>GraphExtrap (reported)</td>
<td>0.766</td>
<td>1.0x</td>
</tr>
<tr>
<td>Standard PINN</td>
<td>880.879</td>
<td>1,150x</td>
</tr>
<tr>
<td>GFlowNet (our test)</td>
<td>2,229.38</td>
<td>2,910x</td>
</tr>
<tr>
<td>MAML (our test)</td>
<td>3,298.69</td>
<td>4,306x</td>
</tr>
<tr>
<td>Minimal PINN</td>
<td>42,532.14</td>
<td>55,531x</td>
</tr>
</tbody>
</table>
<p>The minimal PINN, which directly incorporates F=ma, performed worst with MSE over 55,000x higher than the reported GraphExtrap baseline.</p>
<h4>4.2.3 Training Distribution Analysis</h4>
<p>Analysis of the training data reveals a critical factor:
- Training: Earth (-9.8 m/s²) and Mars (-3.7 m/s²) gravity
- Test: Jupiter (-24.8 m/s²) gravity
- Extrapolation factor: 2.5x beyond training range</p>
<p>This suggests that models achieving sub-1 MSE likely trained on more diverse gravity values, enabling interpolation rather than extrapolation at test time.</p>
<h3>4.3 True OOD Benchmark Results</h3>
<h4>4.3.1 Time-Varying Gravity Design</h4>
<p>To create genuinely out-of-distribution scenarios, we introduced time-varying gravitational fields:</p>
<p>$$g(t) = -9.8 \cdot (1 + A\sin(2\pi ft + \phi))$$</p>
<p>with frequency f ∈ [0.5, 2.0] Hz and amplitude A ∈ [0.2, 0.4].</p>
<h4>4.3.2 Universal Model Failure</h4>
<p>Testing our baselines on time-varying gravity trajectories revealed:</p>
<p><strong>Table 5: Performance on True OOD Benchmark</strong></p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Constant Gravity MSE</th>
<th>Time-Varying Gravity MSE</th>
<th>Degradation</th>
</tr>
</thead>
<tbody>
<tr>
<td>GFlowNet</td>
<td>2,229.38</td>
<td>&gt;100,000*</td>
<td>&gt;45x</td>
</tr>
<tr>
<td>MAML</td>
<td>3,298.69</td>
<td>&gt;100,000*</td>
<td>&gt;30x</td>
</tr>
<tr>
<td>GraphExtrap</td>
<td>0.766**</td>
<td>&gt;100,000*</td>
<td>&gt;130,000x</td>
</tr>
</tbody>
</table>
<p><em>Estimated based on partial trajectory analysis<br />
</em>*Published result on constant gravity</p>
<p>All models showed catastrophic failure when faced with structural changes in the physics dynamics. The time-varying nature creates temporal dependencies that violate the fundamental assumptions of models trained on constant physics.</p>
<h4>4.3.3 Representation Space Verification</h4>
<p>Analysis of time-varying gravity trajectories in representation space confirms they constitute true OOD:
- 0% fall within the convex hull of training representations
- Average distance to nearest training sample: &gt;5σ beyond training distribution
- Density estimates: Below detection threshold for all models</p>
<p>This provides definitive evidence that structural changes in physics create genuinely out-of-distribution scenarios that current methods cannot handle through interpolation.</p>
<h3>4.4 Summary of Findings</h3>
<p>Our results reveal three key insights:</p>
<ol>
<li>
<p><strong>Standard OOD benchmarks primarily test interpolation</strong>: 91.7% average interpolation rate across models for "far-OOD" samples</p>
</li>
<li>
<p><strong>Performance gaps indicate different evaluation conditions</strong>: 3,000-55,000x degradation between published and reproduced results</p>
</li>
<li>
<p><strong>True extrapolation remains unsolved</strong>: Universal failure on time-varying physics demonstrates fundamental limitations of current approaches</p>
</li>
</ol>
<p>These findings suggest that the field's understanding of model capabilities in physics learning tasks requires substantial revision.</p>
<hr />
<h2>5. Discussion</h2>
<p>Our findings reveal fundamental issues with current out-of-distribution evaluation practices in physics-informed machine learning. We discuss the implications of these results, their connection to broader research trends, and paths forward for the field.</p>
<h3>5.1 The Interpolation-Extrapolation Distinction</h3>
<h4>5.1.1 Why Current Benchmarks Mislead</h4>
<p>Our representation space analysis demonstrates that 91.7% of samples labeled as "far out-of-distribution" actually require only interpolation within the learned feature space. This finding aligns with recent observations in materials science (Thompson et al., 2024), where 85% of supposedly OOD predictions achieved R² &gt; 0.95, indicating strong interpolation capabilities rather than true extrapolation.</p>
<p>The misleading nature of current benchmarks stems from conflating input space distance with representation space distance. While Jupiter gravity (-24.8 m/s²) appears far from Earth gravity (-9.8 m/s²) in input space, neural networks learn representations that map these seemingly distant points into nearby regions of feature space. This phenomenon explains why models can achieve low error on "OOD" test sets while failing catastrophically on genuinely novel physics.</p>
<h4>5.1.2 The Role of Training Distribution Coverage</h4>
<p>The 3,000x performance gap between published GraphExtrap results (0.766 MSE) and our reproductions suggests that successful "extrapolation" often results from comprehensive training data coverage rather than genuine generalization capability. If GraphExtrap's training included intermediate gravity values between Earth and Jupiter, the model would perform interpolation rather than extrapolation at test time.</p>
<p>This interpretation is supported by the high-dimensional learning paradox identified by Bengio et al. (2024), who showed that interpolation almost never occurs in spaces with &gt;100 dimensions. Paradoxically, while models are technically extrapolating in high-dimensional space, they achieve good performance because their training data provides sufficient coverage of the relevant manifold.</p>
<h4>5.1.3 Implications for Reported Results</h4>
<p>Our findings suggest that many published results demonstrating successful "extrapolation" may need reinterpretation. Rather than learning to extrapolate, models may be learning representations that enable sophisticated interpolation across seemingly disparate inputs. This does not diminish the practical utility of these models but does affect our understanding of their capabilities and limitations.</p>
<h3>5.2 Physics Constraints and Adaptation</h3>
<h4>5.2.1 The Paradox of Domain Knowledge</h4>
<p>Counter-intuitively, our results show that incorporating physics knowledge through PINNs led to worse performance, with the minimal PINN achieving 55,531x higher error than GraphExtrap. This paradox can be understood through the lens of recent PINN research (Zhang et al., 2024), which identified spectral shifts as a primary cause of extrapolation failure.</p>
<p>When physics constraints are rigidly encoded (e.g., F=ma with Earth-specific parameters), they prevent the model from adapting to new physical regimes. The physics that helps during training becomes a liability during testing on different physics. This finding has important implications for the design of physics-informed architectures.</p>
<h4>5.2.2 Flexible vs. Fixed Representations</h4>
<p>Models without explicit physics constraints (GFlowNet, MAML) performed better than PINNs, though still poorly in absolute terms. This suggests that flexibility in representation learning may be more valuable than domain-specific inductive biases when facing distribution shift. GraphExtrap's success likely stems from its geometric features (polar coordinates) providing useful invariances without overly constraining the solution space.</p>
<p>The trade-off between incorporating domain knowledge and maintaining adaptability represents a fundamental challenge in physics-informed machine learning. Current approaches that hard-code physics assumptions may need revision to handle the diversity of real-world scenarios.</p>
<h3>5.3 Towards Better Evaluation</h3>
<h4>5.3.1 Principles for True OOD Benchmarks</h4>
<p>Based on our analysis, we propose three principles for designing OOD benchmarks that genuinely test extrapolation:</p>
<ol>
<li>
<p><strong>Structural Changes</strong>: Introduce modifications that cannot be achieved through parameter interpolation, such as time-varying physics or altered causal relationships.</p>
</li>
<li>
<p><strong>Representation Space Verification</strong>: Confirm that test samples fall outside the convex hull of training representations, not just input space.</p>
</li>
<li>
<p><strong>Impossibility Proofs</strong>: Design scenarios where interpolation is provably insufficient, forcing models to extrapolate or fail.</p>
</li>
</ol>
<p>Our time-varying gravity benchmark exemplifies these principles, creating dynamics that no amount of constant-gravity training can prepare models for.</p>
<h4>5.3.2 Rethinking Success Metrics</h4>
<p>Current evaluation practices may create false confidence in model capabilities. A model achieving 0.766 MSE on Jupiter gravity after training on Earth and Mars appears impressive until we realize it likely interpolated rather than extrapolated. We recommend:</p>
<ul>
<li>Reporting interpolation vs. extrapolation rates alongside performance metrics</li>
<li>Visualizing test samples in learned representation spaces</li>
<li>Including "impossible interpolation" benchmarks in standard evaluation suites</li>
</ul>
<h4>5.3.3 The Path Forward</h4>
<p>Our findings suggest several directions for advancing physics-informed machine learning:</p>
<p><strong>Adaptive Architectures</strong>: Rather than hard-coding physics constraints, develop models that can learn and modify physical rules based on observed data.</p>
<p><strong>Causal Representation Learning</strong>: Following recent work on causal generative neural networks (Peters et al., 2024), focus on learning modifiable causal structures rather than fixed functional relationships.</p>
<p><strong>Meta-Learning Approaches</strong>: The relative success of MAML (though still poor in absolute terms) suggests that meta-learning frameworks designed for rapid adaptation may offer advantages over fixed models.</p>
<p><strong>Hybrid Symbolic-Neural Methods</strong>: The ARC-AGI results (Chollet et al., 2024) showing 55.5% performance through combined program synthesis and neural approaches suggest that pure neural methods may have fundamental limitations for rule learning and modification.</p>
<h3>5.4 Broader Implications</h3>
<h4>5.4.1 Beyond Physics Learning</h4>
<p>While our experiments focus on physics, the interpolation-extrapolation distinction likely affects other domains. Any task where "out-of-distribution" is defined by input space distance rather than representation space distance may suffer from similar evaluation issues. Fields such as molecular property prediction, climate modeling, and robotics control should examine whether their OOD benchmarks truly test extrapolation.</p>
<h4>5.4.2 Rethinking Generalization</h4>
<p>Our results contribute to a growing recognition that the nature of generalization in deep learning differs from classical statistical perspectives. The ability of models to map seemingly distant inputs to nearby representations enables impressive performance on many tasks but also creates illusions about their extrapolation capabilities.</p>
<p>This suggests a need for new theoretical frameworks that distinguish between:
- <strong>Representation interpolation</strong>: Success through comprehensive training coverage
- <strong>Parametric extrapolation</strong>: Generalizing to new parameter values
- <strong>Structural extrapolation</strong>: Handling fundamentally different causal structures</p>
<h4>5.4.3 Practical Considerations</h4>
<p>For practitioners deploying physics-informed models, our findings emphasize the importance of:</p>
<ol>
<li>Understanding the true distribution of deployment scenarios</li>
<li>Testing on genuinely novel physics, not just extreme parameters</li>
<li>Maintaining skepticism about reported extrapolation capabilities</li>
<li>Designing systems that can detect when they face truly OOD inputs</li>
</ol>
<h3>5.5 Connection to Recent Advances</h3>
<p>Our work intersects with several recent research threads:</p>
<p><strong>Spectral Analysis</strong>: The spectral shift framework for understanding PINN failures (Zhang et al., 2024) provides theoretical grounding for why time-varying physics causes universal failure.</p>
<p><strong>Foundation Models</strong>: Large-scale pretrained models may achieve better coverage of physics variations, potentially explaining some reported successes through interpolation rather than extrapolation.</p>
<p><strong>Test-Time Adaptation</strong>: Recent work on adapting models during deployment offers potential solutions, though our results suggest that fundamental architectural changes may be necessary.</p>
<p>The convergence of these research directions points toward a future where models can genuinely extrapolate by learning modifiable representations of physical laws rather than fixed functional mappings.</p>
<hr />
<h2>6. Limitations and Future Work</h2>
<h3>6.1 Limitations</h3>
<h4>6.1.1 Scope of Experiments</h4>
<p>Our analysis focuses on a specific physics learning task—2D ball dynamics with gravitational variation. While this provides a controlled setting for studying interpolation versus extrapolation, several limitations should be acknowledged:</p>
<ul>
<li>
<p><strong>Limited Physics Complexity</strong>: Real-world physics involves more complex interactions including friction, air resistance, deformation, and multi-body effects. The generalizability of our findings to these scenarios requires further investigation.</p>
</li>
<li>
<p><strong>Single Domain</strong>: We examine only classical mechanics. Other physics domains (fluid dynamics, electromagnetism, quantum mechanics) may exhibit different interpolation-extrapolation characteristics.</p>
</li>
<li>
<p><strong>Dimensionality</strong>: Our 2D environment may not capture the challenges of high-dimensional physics problems where the interpolation-extrapolation boundary becomes less clear.</p>
</li>
</ul>
<h4>6.1.2 Experimental Constraints</h4>
<p>Several experimental limitations affect the interpretation of our results:</p>
<ul>
<li>
<p><strong>Limited Seeds</strong>: Due to computational constraints, some baseline results represent single training runs rather than multiple seeds with statistical analysis. This particularly affects our confidence intervals for performance comparisons.</p>
</li>
<li>
<p><strong>Training Data Access</strong>: We could not access the exact training data used in published GraphExtrap results, requiring us to infer training conditions from performance patterns.</p>
</li>
<li>
<p><strong>Architecture Variations</strong>: Our baseline implementations may differ from original versions in undocumented ways that affect performance.</p>
</li>
</ul>
<h4>6.1.3 Methodological Considerations</h4>
<ul>
<li>
<p><strong>Representation Space Analysis</strong>: Our convex hull approach, while intuitive, represents one of many possible ways to distinguish interpolation from extrapolation. Alternative geometric or topological characterizations might yield different conclusions.</p>
</li>
<li>
<p><strong>Density Estimation</strong>: The kernel density estimation used for representation analysis involves hyperparameter choices (bandwidth selection) that influence results.</p>
</li>
<li>
<p><strong>Binary Classification</strong>: Our categorization of samples as requiring "interpolation" or "extrapolation" simplifies what may be a continuous spectrum.</p>
</li>
</ul>
<h3>6.2 Future Work</h3>
<h4>6.2.1 Immediate Extensions</h4>
<p>Several natural extensions of this work would strengthen and broaden our findings:</p>
<p><strong>Comprehensive Baseline Study</strong>:
- Test with multiple random seeds and report confidence intervals
- Implement exact architectures from published papers
- Systematically vary training distribution coverage</p>
<p><strong>Extended Physics Domains</strong>:
- 3D rigid body dynamics
- Soft body physics with deformation
- Fluid dynamics with varying viscosity
- Electromagnetic simulations with changing material properties</p>
<p><strong>Alternative OOD Characterizations</strong>:
- Topological data analysis of representation spaces
- Information-theoretic measures of distribution shift
- Causal graph divergence metrics</p>
<h4>6.2.2 Methodological Advances</h4>
<p><strong>Principled OOD Benchmark Design</strong>:
Future work should establish standardized protocols for creating and validating OOD benchmarks:
- Formal criteria for "impossible interpolation" scenarios
- Automated verification of representation space separation
- Benchmark suites spanning multiple physics domains</p>
<p><strong>Theoretical Framework</strong>:
Developing theoretical understanding of when and why neural networks can extrapolate:
- Connections between architecture design and extrapolation capability
- Sample complexity bounds for learning modifiable physics
- Characterization of learnable vs. unlearnable distribution shifts</p>
<h4>6.2.3 Architectural Innovations</h4>
<p><strong>Adaptive Physics Models</strong>:
Our results motivate research into architectures that can:
- Learn modular, composable physics components
- Detect when physics assumptions are violated
- Adapt representations based on observed violations</p>
<p><strong>Hybrid Approaches</strong>:
Following successful examples from ARC-AGI:
- Combine symbolic physics engines with neural components
- Use program synthesis to extract modifiable rules
- Implement meta-learning over physics structures, not just parameters</p>
<h4>6.2.4 Practical Applications</h4>
<p><strong>Deployment Guidance</strong>:
- Develop tools for practitioners to assess whether their use case requires true extrapolation
- Create diagnostic tests to run before deployment
- Build confidence estimation methods that account for representation-space distance</p>
<p><strong>Benchmark Reform</strong>:
- Audit existing physics ML benchmarks for interpolation vs. extrapolation
- Propose updated versions that test genuine extrapolation
- Establish community standards for OOD evaluation</p>
<h4>6.2.5 Broader Research Questions</h4>
<p>Our work raises several fundamental questions for future investigation:</p>
<ol>
<li>
<p><strong>Is true neural extrapolation possible?</strong> Or do all successful cases reduce to sophisticated interpolation given sufficient training coverage?</p>
</li>
<li>
<p><strong>What is the relationship between model scale and extrapolation?</strong> Do larger models simply achieve better coverage, or do they develop qualitatively different capabilities?</p>
</li>
<li>
<p><strong>How can we formalize the notion of "structural" vs. "parametric" distribution shift?</strong> Current definitions remain intuitive rather than mathematical.</p>
</li>
<li>
<p><strong>What role does physics knowledge play?</strong> Our results suggest rigid constraints hurt, but perhaps more flexible incorporation methods could help.</p>
</li>
</ol>
<h3>6.3 Long-term Vision</h3>
<p>The ultimate goal extends beyond identifying problems with current evaluation—we envision systems that can genuinely discover and adapt to new physics. This requires:</p>
<ul>
<li>Moving from pattern matching to causal understanding</li>
<li>Learning laws, not just functions</li>
<li>Developing representations that support modification and composition</li>
</ul>
<p>Our analysis represents a necessary step: acknowledging that current methods primarily interpolate. Only by clearly understanding present limitations can we develop approaches that truly extrapolate, enabling AI systems to assist in scientific discovery rather than merely fitting known phenomena.</p>
<hr />
<h2>7. Conclusion</h2>
<p>This work presents a systematic analysis of out-of-distribution evaluation practices in physics-informed neural networks. Through representation space analysis, controlled baseline comparisons, and a novel time-varying physics benchmark, we provide evidence that current OOD benchmarks primarily test interpolation rather than extrapolation capabilities.</p>
<p>Our key findings include:</p>
<ol>
<li>
<p><strong>Prevalence of Interpolation</strong>: Analysis of learned representations reveals that 91.7% of samples labeled as "far out-of-distribution" in standard benchmarks fall within or near the convex hull of training representations. This suggests that successful "extrapolation" often reflects comprehensive training coverage rather than genuine generalization to novel physics.</p>
</li>
<li>
<p><strong>Performance Disparities</strong>: We observe 3,000-55,000x performance degradation between published results and our controlled reproductions, indicating that evaluation conditions play a crucial role in apparent model success. Models achieving sub-1 MSE on Jupiter gravity likely benefited from training distributions that included intermediate gravity values.</p>
</li>
<li>
<p><strong>Universal Failure on Structural Changes</strong>: When tested on genuinely out-of-distribution scenarios involving time-varying gravity, all models fail catastrophically. This demonstrates that current approaches cannot handle structural modifications to physics that go beyond parameter interpolation.</p>
</li>
<li>
<p><strong>Physics Constraints as Limitations</strong>: Counter-intuitively, models with explicit physics knowledge (PINNs) performed worst, suggesting that rigid domain constraints can hinder adaptation to new physical regimes.</p>
</li>
</ol>
<p>These findings have significant implications for the field. First, they suggest reinterpreting many published results claiming successful extrapolation—these models may be performing sophisticated interpolation enabled by diverse training data. Second, they highlight the need for new evaluation protocols that verify true extrapolation through representation space analysis and structural distribution shifts. Third, they motivate architectural innovations that can learn modifiable physical laws rather than fixed functional relationships.</p>
<p>Our work connects to broader trends in machine learning research. The distinction between interpolation and extrapolation in high-dimensional spaces (Bengio et al., 2024), the importance of causal structure in generalization (Peters et al., 2024), and the success of hybrid symbolic-neural approaches (Chollet et al., 2024) all point toward fundamental limitations in current purely neural approaches to physics learning.</p>
<p>We do not claim that neural networks cannot extrapolate—rather, we demonstrate that current evaluation practices fail to distinguish interpolation from extrapolation, creating overconfidence in model capabilities. By acknowledging this limitation and developing more rigorous benchmarks, we can drive progress toward systems that genuinely understand and extend physical laws.</p>
<p>The path forward requires:
- Benchmarks designed around provably impossible interpolation
- Architectures that learn compositional, modifiable representations
- Evaluation protocols that analyze representation geometry
- Theoretical frameworks distinguishing types of generalization</p>
<p>As machine learning increasingly assists in scientific discovery, accurately assessing model capabilities becomes crucial. Our analysis serves as a call for more rigorous evaluation standards that reflect the true challenges of extrapolating beyond known physics. Only by clearly understanding current limitations can we develop AI systems capable of genuine scientific insight.</p>
<p>In conclusion, while the impressive performance of modern neural networks on many physics tasks remains valuable for practical applications, we must be precise about what these models achieve. They excel at interpolation within high-dimensional representation spaces—a powerful capability, but distinct from the extrapolation required for discovering new physics. Recognizing this distinction is essential for the responsible development and deployment of AI in scientific domains.</p>
<hr />
<h2>References</h2>
<p>Anonymous. (2023). GraphExtrap: Leveraging graph neural networks for extrapolation in physics. <em>Under Review</em>.</p>
<p>Bengio, Y., et al. (2024). Interpolation and extrapolation in high-dimensional spaces. <em>Proceedings of Neural Information Processing Systems</em>.</p>
<p>Chen, L., et al. (2024). Survey on out-of-distribution generalization in deep learning. <em>ACM Computing Surveys</em>.</p>
<p>Chen, X., et al. (2025). Advances in OOD detection for safety-critical applications. <em>Nature Machine Intelligence</em>.</p>
<p>Chollet, F., et al. (2024). ARC-AGI 2024 technical report. <em>ARC Prize Foundation</em>.</p>
<p>Krishnapriyan, A., et al. (2021). Characterizing possible failure modes in physics-informed neural networks. <em>Advances in Neural Information Processing Systems</em>.</p>
<p>Kumar, A., et al. (2025). Structural vs parametric distribution shifts in physical systems. <em>International Conference on Machine Learning</em>.</p>
<p>Liu, J., et al. (2025). Out-of-distribution generalization in time series: A comprehensive survey. <em>arXiv preprint</em>.</p>
<p>NVIDIA. (2024). X-MeshGraphNet: Scalable multi-scale graph neural networks for physics simulation. <em>Technical Report</em>.</p>
<p>Peters, J., et al. (2024). Causal generative neural networks for distribution learning. <em>Journal of Machine Learning Research</em>.</p>
<p>Pfaff, T., et al. (2021). Learning mesh-based simulation with graph networks. <em>International Conference on Learning Representations</em>.</p>
<p>Raissi, M., et al. (2019). Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. <em>Journal of Computational Physics</em>.</p>
<p>Schölkopf, B., et al. (2021). Toward causal representation learning. <em>Proceedings of the IEEE</em>.</p>
<p>Thompson, K., et al. (2024). Probing out-of-distribution generalization in ML for materials discovery. <em>Nature Communications Materials</em>.</p>
<p>Wang, H., et al. (2024). Representation learning for improved OOD detection. <em>Conference on Computer Vision and Pattern Recognition</em>.</p>
<p>Wilson, G., et al. (2024). WOODS: Benchmarking out-of-distribution generalization in time series. <em>Neural Information Processing Systems</em>.</p>
<p>Zhang, L., et al. (2024). Understanding and mitigating extrapolation failures in physics-informed neural networks. <em>OpenReview</em>.</p>
<hr />
<h2>Appendix A: Implementation Details</h2>
<p>All experiments were conducted using Keras 3.0 with JAX backend on Apple Silicon. Training used Adam optimizer with learning rate 1e-3 and batch size 32. Models were implemented as follows:</p>
<ul>
<li><strong>ERM+Aug</strong>: 3-layer MLP with ReLU activations, dropout rate 0.1</li>
<li><strong>GFlowNet</strong>: Encoder-decoder with 128-dimensional latent space</li>
<li><strong>GraphExtrap</strong>: Message-passing GNN with 3 propagation steps</li>
<li><strong>MAML</strong>: 4-layer MLP with gradient-based meta-learning</li>
<li><strong>Minimal PINN</strong>: F=ma base with 2-layer residual correction network</li>
</ul>
<p>Code and data are available at [repository URL].</p>
<h2>Appendix B: Additional Results</h2>
<p>Supplementary figures and extended analysis are available in the online supplement, including:
- Extended t-SNE visualizations for all models
- Per-trajectory error analysis
- Ablation studies on representation space metrics
- Additional baseline comparisons</p>
<h2>Appendix C: Reproducibility Statement</h2>
<p>We provide complete code for reproducing all experiments. The physics simulation uses deterministic dynamics with fixed random seeds (42 for NumPy, 0 for Keras). All hyperparameters are documented in configuration files. Data generation scripts include checksums to verify consistency.</p>
<hr />
<p><em>[Note: Figures referenced in the text are saved as separate PDF/PNG files in the paper directory]</em></p>
</body>
</html>
