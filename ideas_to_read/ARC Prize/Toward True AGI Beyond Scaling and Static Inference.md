# Toward True AGI: Beyond Scaling and Static Inference
## Summary
The below text taken from the Lecture by Francois Chollet (creator or Keras) on YouTube: https://www.youtube.com/watch?v=5QcCeSsNRks  explores the progression of Artificial Intelligence towards Artificial General Intelligence (AGI), highlighting that a fall in computing costs and an abundance of data initially propelled deep learning and large language models (LLMs). However, it argues that scaling alone is insufficient for achieving true fluid intelligence, which involves the ability to adapt to novel situations, as opposed to merely memorising skills. The author proposes "test-time adaptation" as a crucial shift in AI research, enabling models to dynamically modify their behaviour, and introduces the Abstraction and Reasoning Corpus (ARC) benchmarks (ARC-1, ARC-2, and the upcoming ARC-3) as tools to measure this fluid intelligence. Ultimately, the text advocates for an AI architecture that combines "type one" (continuous, intuition-based) and "type two" (discrete, program-centric) abstractions, guided by deep learning and discrete program search, to foster genuine invention and accelerate scientific progress.

## Links
ARC Prize: https://www.kaggle.com/competitions/arc-prize-2025/overview


## Text
hi everyone I'm Francois i'm super excited to share with you some of my ideas about HGI and how we're going to get there this chart right there is one of the most important facts about the world the cost of compute has been consistently falling by two orders of magnitude every decade since 1940 there's no sign that is stopping anytime soon and in AI compute and data have long been the primary bottleneck to what we could achieve and in 2010s as you all know with the abundance of GPU based compute and large data sets deep learning really started to work and all of a sudden we are making fast progress on problems that had long seemed intractable across computer vision and natural language processing and in particular self-supervised text modeling started to work and the dominant paradigm of AI became scaling up LM3 training and this approach was crushing almost all benchmarks and remarkably it was getting predictably better benchmark results as we scaled up model size and train data size with the exact same architecture and the exact same training process that's the scaling laws that Jared told you about a few minutes ago so it really seemed like really it all figured out and many people extrapolated that more scale uh was all that was needed to solve everything and get to a GI our field became obsessed with the idea that general intelligence would spontaneously emerge by cramming more and more data into bigger and bigger models but there was one problem we were confused about what these benchmarks really meant there's a big difference between memorized skills which are static and task specific and fluid general intelligence the ability to understand something you've never seen before on the fly and back in 2019 before the rise of LLMs I released an AI benchmark to highlight this difference uh it's called the abstraction reasoning corpus or ark1 and from uh at that time back in 2019 to now with a model like GP4.5 for instance there's been a roughly 50,000x scale up of basel and we went from 0% accuracy on that benchmark to roughly 10% which is not a lot it's very close to zero if you take into account the fact that any one of you in this room would score well above 95% so to crack general fluid intelligence it turns out we needed new ideas beyond just scaling up pre-training and doing static inference this benchmark was not about uh regurgitating memorized skills it was really about making sense of a new problem that you've never seen before on the fly but then last year in 2024 everything changed the AI research community started pivoting to a new and very different pattern test adaptation creating models that could change their own state at test time to adapt to something new so this wasn't about quering pre-loaded knowledge anymore it was really about the ability to learn and adapt at inference time and suddenly we started seeing significant progress on ARC so finally we had AI that was showing genuine signs of fluid intelligence so in particular in December last year OpenAI previewed its uh 03 model and they used a version of it that was uh fine-tuned specifically on ARC and that showed human level performance on that benchmark to the first time and today in 2025 we have suddenly moved on from the pre-training scaling pattern and we're now fully in the era of Tesla adaptation so Tesla adaptation is all about the ability of a model to modify its own behavior dynamically based on the specific data it encounters during inference so that covers techniques like test time training program synthesis train of thought synthesis where the model tries to reprogram itself uh for the task at hand and today every single AI approach that performs well on ARC is using one of these techniques so today I want to answer the following questions first why did the pre-training scaling paradigm not get us to a GI if you look back just two years ago this was the standard dogma everybody was saying this and today almost no one believes this anymore so what happened and next does this adaptation get us to AGI this time and if that's the case maybe AGI is already here some people believe so and finally besides uh the sign adaptation what else might be next for AI and to answer these questions we have to go back to a more fundamental question what is even intelligence what what do we mean when we say we're trying to build AGI if you look back over the past decades there's been two lines of thoughts to define intelligence and to define the goals of AI there's the Minsky style view uh AI is about making machines that are capable of performing tasks uh that would normally be done by humans and this echoes very closely uh the current mainstream corporate view that AGI would be uh a model that could perform most economically valuable tasks like 80% is often quoted as as the number but then there's the mac view that AI is about getting machines to handle problems they have not been prepared for it's about getting AI to deal with something new and my view is more like the MATI view intelligence is a process and skill is the output of that process so skill itself is not intelligence and displaying skill at any number of tasks does not show intelligence this is like the difference between a road network and a road building company if you have a road network then you can go from A to B for a specific predefined set of A's and B's but if you have a road building company then you can start connecting new A's new B's on the fly as your needs evolve so intelligence is the ability to deal with new situations it's the ability to blaze fresh trails and build new roads so attributing intelligence towards really a crystallized behavior program a skill program that's a category error you are confusing the process and its output so don't confuse the road and the process that created the road so to formalize this a bit I see intelligence as the conversion ratio between the information you have uh mostly your past experience but also any uh developer imparted prior that the system might have and your operational area over the space of potential future situations that you might encounter and that's going to feature high novelty and uncertainty so intelligence is the efficiency with which you operationalize past information in order to deal with the future it's an efficiency ratio and that's the reason why using exam like benchmarks models is a bad idea they're not going to tell you how close we are to AGI because human exams weren't designed to measure intelligence they were designed to measure task specific skill and knowledge they were designed according to assumptions that are sensible for humans but not for machines like for instance most exams assume that you haven't read and memorized all the exam questions and the answers beforehand so if you want to regly define and measure intelligence here are some key concepts that you have to take into account the first is the distinction between static skills and fluid intelligence so between having access to a collection uh of static programs to solve known problems versus being able to synthesize brand new programs on the fly to face a problem you never seen before and of course it's not a binary it's not one or the other there's a spectrum between the two the second concept is operational area for a given skill there's a big difference between being skilled only in situations that are very close to what you've seen before and uh being skilled for any situation within a very broad scope for instance if you know how to drive you should be able to drive in any city not just uh in a specific geopence area like you can learn to drive in San Jose and then move to Sacramento and you can still drive right again so it's there's a spectrum there it's not it's not binary and lastly you should look at uh information efficiency for a given skill how much information how much data how much practice did you need to acquire that skill and of course higher information efficiency means higher intelligence and the reason these definitions matter a lot is that as engineers we can only build what we measure so the way we define and measure intelligence is not a technical detail it really reflects our understanding of the problem of cognition it scopes out the the questions we're going to be asking and so it determines the answers that we're going to be getting it's the feedback signal that drives us towards our goals and a phenomenon you see constantly in engineering is the shortcut rule so it's it's the fact that when you focus on uh achieving a single measure of success you may succeed but you will do that at the expense of everything else that was not captured by your measure so you hit the target but you miss the points and you see this all the time uh on Kaggle for instance uh we saw it with the Netflix prize where the winning system was extremely accurate but it was way too complex to ever be used in production so it ended up never being used it was effectively pointless uh we also saw it in AI uh with chess playing for AI the reason the AI community set out to create uh programs that could play chess back in the 70s was because uh people expected this would teach us about human intelligence and then a couple decades later we achieved a goal when Deep Blue beat Kasparov the world champion and in the process we had really learned nothing about intelligence so you hit the targets but you miss the points and for decades AI has chased task specific skill because that was our definition of intelligence but this definition only leads to automation which is exactly the kind of system that we have today but we actually want AI that's capable of autonomous invention we don't want to stop at automating known tasks we want AI that could tackle humanity's most difficult challenges and accelerate scientific progress that's what AGI is meant to be and to achieve that we need a new target we need to stop targeting fluid intelligence itself the ability to adapt and invent so one definition of AGI only enops automation so it increases economic productivity obviously it's extremely valuable maybe it also increases unemployment but the other definition unlocks invention and the acceleration of the timeline of science and it's by measuring what you really care about that we'll be able to make progress so we need a better target we need a better feedback signal what does that look like my first attempt at creating a way to measure intelligence in AI systems was the RKGI benchmark so I released ARK1 back in uh 2019 it's like an IQ test for machines and also humans so ARK1 contains 1,000 tasks like this one here and each task is unique so that means that you cannot cram for ARC you have to figure out each task on the fly by using your general intelligence rather than your memorized knowledge and of course solving any problem always requires some knowledge and in the case of most benchmarks the knowledge prior that you need are typically left implicit in the case of ARC we made them explicit so all ARC tasks are built entirely on top of core knowledge prior which are things like objectness uh elementary physics um basic geometry topology counting so concepts that any fouryear-old child has already mastered and solving arc requires very little knowledge and it's knowledge that is very much not specialized so you don't need to prepare for arc in order to solve it and what makes arc unique is that you cannot solve it purely by memorizing patterns it really requires you to demonstrate through the intelligence and meanwhile pretty pretty much every other benchmark out there is targeting fixed known tasks so they can't actually be solved or hacked via memorization alone that's what makes ARC fairly easy for humans but very challenging for AI and when you see a problem like this where a human child can perform really well but the most advanced the most sophisticated AI models out there struggle that's like a big red flashing light telling you that we're missing something that new ideas are needed one thing I want you to keep in mind is that ARC is not going to tell you whether a system is already a GR or not that's not its purpose arc is really a tool to direct the attention of the research community towards what we see as the most important unsolved bottlenecks on the way to AGI so ARC is not the destination and solving ARC is not the goal arc is really just an arrow pointing in the right direction and ARC has completely resisted the pre-training scaling paradigm even after a 50,000x scale up of pre-trained baselons their performance on ARC stayed near zero so we can decisively conclude that fluid intelligence does not emerge from scaling up pre-training you absolutely need test adaptation in order to demonstrate genuine fluid intelligence and importantly when the arrival of test adaptation happened last year ARC was really the only benchmark at the time that provided a clear signal about the profound shift that was happening other benchmarks were saturated so they could not distinguish between a true IQ increase and just brute force scaling so now you you see this graph and you're probably asking well clearly at this point AR1 is also saturating so does that mean we have human level AI now well not yet what you see on this graph is that ARK1 was a binary test it was a minimal reproduction of fluid intelligence so it only really gives you two possible modes either you have no fluid intelligence in which case you will score near zero like basel or you have nonzero fluid intelligence in which case you will instantly score very high like the O3 model from open eye for instance and of course every one of you in this room would score within noise distance of 100% so ARC saturates ARK one saturates way below human level fluid intelligence and so now we are in need of a better tool a more sensitive tool that would provide more useful bandwidths and better comparison with human intelligence and that tool is ARGI 2 which released in March this year so back in 2019 ARK1 was meant to challenge the deep learning pattern where models are big parametric curves uh used for static inference and today ARK 2 challenges reasoning systems it changes the test adaptation pattern the benchmark format is still the same there's a much greater focus on probing compositional jarization so the tasks are still very feasible for humans but they're much more sophisticated and as a result ARK 2 is not easily brute forceable in practice what this means is that in ARK one for many tasks you could just look at it and instantly see the solution without having to think too much about it with ARK 2 all tasks require some level of deliberate thinking but they still remain very feasible for humans and we know this because we tested 400 people firsthand in person in San Diego over several days and we are not talking about people who have physics PhDs here we recruited uh random folks uh Uber drivers UCDS students uh people who are unemployed so basically anyone trying to make some money on the side and all tasks in AR 2 were sold by at least two of the people that saw it and each task was seen on average by about seven people and so what that tells you is that a group of 10 random people with majority voting would score 100% on ARK 2 so we know these tasks are completely doable by regular folks with no prior training so how well do AI models do well if you take basel models like GPT4.5 Lama 4 it's simple they get 0% there is simply no way to do these tasks simply via memorization next if you look at static reasoning systems so systems that use a single chin of thoughts that they generate for the task they don't do much better they do on the order of one to 2% so very much within noise distance of zero so what that tells you is that to solve AR 2 you really need test adaptation all systems that do meaningfully above zero are using TTL but even then they're still far below human level so compared to ARK1 AR2 enables much more granular evaluation of DTS systems systems like 03 for instance and that's where you see that 03 and other systems like it are still not yet quite human level and in my view as long as it's easy to come up with tasks that any one of you can do that are easy for humans but that AI cannot figure out no matter how much computers are it we don't have a GI yet and you will know that we are close to having AGI when it becomes increasingly difficult to come up with such tasks we're clearly not there yet and to be clear I don't think ARK 2 is the final test we're not going to stop at ARC 2 we've started development on RKGI 3 and AR 3 is a significant departure from the input output pair formats of ARC one and two we're assessing agency the ability to explore uh to learn interactively to set goals uh achieve goals autonomously so your AI is dropped into a brand new environment where it doesn't know what the controls do it doesn't know what the goal is it doesn't know where the the gameplay mechanics are just to figure out everything on the fly starting with what is it even supposed to do uh uh in the game and every single game is entirely unique they're all built on top of core knowledge prior only just like in AR one and two so we'll have hundreds of interactive reasoning tasks like this one and efficiency is central to the design of AR 3 so models won't just be graded on whether they can solve a task but on how efficiently they solve it and we are establishing a strict limit of the number of actions that a model can take and we are targeting the same level of action efficiency as we observe in human so we're going to launch this in early 2026 and next month in July we're going to release a developer preview so you can start playing with it what's it going to take to solve AR 2 and we're still very far from it today uh then solve AR 3 and we're even further away from that maybe in the future solve AR 4 eventually get to AGI what are we still missing so I've said that intelligence is the efficiency with which you operationalize the past to face a constantly changing future but of course if the future you face had really nothing in common with the past no common ground with anything you've seen before you could not make sense of it no matter how intelligent you were but here's the thing nothing is ever truly novel the universe around you is made of many different things that are all similar to each other like one tree is similar to another tree is also similar to your neuron or electromagnetism is similar to hydrodnamics is also similar to gravity so we are surrounded by isomorphisms i call this the kaleidoscope hypothesis our experience of the world seems to feature a neverending novelty and complexity but the number of unique atoms of meaning that you need to describe it is actually very small and everything around you is a recombination of these atoms and intelligence is the ability to mine your experience to identify these atoms of meaning that can be reused across many different situations across many different tasks and this involves identifying um invariance uh structure uh things that seem to be repeated principles and these building blocks these atoms are called abstractions and whenever you encounter a new situation you're going to make sense of it by recombining on the fly abstractions from your collection to create a brand new model that's adapted to the situation so implementing intelligence is going to have two key parts first there's abstraction acquisition you want to be able to efficiently extract reusable abstractions from your past experience from a feed of data for instance and then there's on the-fly recombination you want to be able to efficiently uh select and recombine these building blocks uh into models that are fit for the current situation and the emphasis on efficiency here is crucial how intelligent you are is not just determined by whether you can do something it's determined by how efficiently you can acquire good abstractions from ret experience how efficiently you can recombine them to navigate novelty so if you need hundreds of thousands of hours to acquire a simple skill uh you're not very intelligent or if you need to enumerate every single move on the chessboard to find the best move you're not very intelligent so intelligence is not just demonstrating high skill it's really the efficiency with which you acquire and deploy these skills it's both data efficiency and compute efficiency and at this point you start to see why simply making our AI models bigger and training them on more data didn't automatically lead to a GI we were missing a couple of things first these models lacked the ability to do on the-fly recombination so at training time they were learning a lot they were acquiring many useful abstractions but then at test time they were completely static you could only use them to fetch and apply a pre-recorded template and that is a critical problem that test adaptation is addressing tta adds on the re combination capabilities to our AI and that's actually that's a huge step forward that gets us much much closer to a GI that's not the only problem recombination is not the only thing missing the other problem is that these models are still incredibly inefficient if you take gradient descent for instance gradient descent uh requires vast amounts of data to distill simple abstractions many orders of magnitude more data than what humans need roughly three to four orders of magnitude more and if you look at uh uh re combination efficiency even the latest state-of-the-art CTA techniques they still need thousands of dollars of compute uh to solve ARK1 at human level and that doesn't even scale to AR 2 and the fundamental issue here is that deep learning models are missing compositional generalization and that's the thing that ARK 2 is trying to measure and the reason why is that there's more than one kind of abstraction and this is really important i said that intelligence is about mining abstractions from data and then re combining them there's really two kinds of abstraction there's type one and type two they're pretty similar to each other they mirror each other so both are about comparing things comparing instances and merging individual instances into common templates by eliminating certain details about the instances so basically you take a bunch of things you compare them you drop the details that don't matter and what you're left with is an abstraction and the key difference between the two is that one operates over a continuous domain and the other operates over a discrete domain so type one or value centric abstraction is about comparing things via a continuous distance function and that's the kind of abstraction that's behind uh perception pattern cognition intuition and also of course modern machine learning and type two or programcentric abstraction is about comparing discrete programs which is to say graphs and instead of trying to compute distances between them you're going to be looking for uh exact structure matching You're going to be looking for exact isomorphisms subgraph isomeorphisms and this is what underlying much of human reasoning it's also what software engineers do when they're refactoring some code so if you hear a software engineer talk about abstraction they mean this kind of abstraction so two kinds of abstraction both driven by analogy making either value analogy or program analogy and all cognition arises from a combination of these two forms of abstraction you can remember them they had left brain versus right brain metaphor one half for perception intuition and the other half for reasoning planning uh rigor and transformers are greats at type one abstraction they can do everything that type one is effective for perception intuition pattern cognition they all work well so in that sense transformers are major breakthrough in AI but they're still not a good fit for type two and this is why you will struggle to train one of these models to do very simple type two things like sorting a list or adding digits provided as a sequence of tokens so how are we going to get to type two you have to leverage discrete program search as opposed to purely manipulating continuous interpolated spaces learn with descent search is what unlocks invention beyond just automation all known AI systems today that are capable of some kind of uh uh invention some kind of creativity they rely on discrete search uh even back in the 90s we were already using gigantic search to come up with new antenna designs or you can take uh Alph Go with move 37 that was discrete search or more recently the alpha evol system from deep mind all discrete search systems so deep learning doesn't invent but search does so what's discrete program search it's basically combinatoral search over graphs of operators taken from some language some DSL and to better understand it you can try to draw an analogy between program synthesis and the machine learning techniques you already know about in machine learning your model is a differentiable parametric function so it's a curve in program synthesis it's going to be a discrete graph a graph of ops symbolic ops from some language in ML your learning engine the way you create models is gradient descent which is very comput efficient by the way gradient descent will let you find a model that fits the data very quickly very efficiently in program synthesis the learning engine is searchal search which is extremely compute efficient obviously in machine learning the key obstacle that you run into is data density in order to fit a model you need a dense sampling of the data manifold you need a lot of data and program synthesis is the exact reverse program synthes is extremely data efficient you can fit a program using only two or three examples but in order to find that program you have to sift through a vast space of potential programs and the size of that space grows cuminally with problem complexity so you run into this communatoral explosion wall i said earlier that intelligence is a combination of two forms abstraction type one and type two and I really don't think that you're going to go very far if you go all in on just one of them like all in on type one or all in on type two i think that if you want to really unlock their potential you have to combine them together and that's what human intelligence is really good at that's really what makes us special we combine perception and intuition together with explicit step-by-step reasoning we combine both forms of abstraction in all our thoughts all our actions everywhere for instance when you're playing chess you're using type two when you calculate when you unfold some potential moves step by step in your mind but you're not going to do this for every possible move of course because there are too many of them right you're only going to be doing it for a couple of different options right like here you're going to look at the knight the queen and the way you narrow down these options is via intuition is via pattern recognition on the board so and you build that up very much through experience right you've mined your past experience unconsciously to extract these patterns and that's very much type one so you're using type one intuition to make type two calculation tractable so how is the merger between type one and type two going to work well the key system two technique is discrete search over a space of program one and the blocker that you run into is explosion and meanwhile the key system one technique is uh curve fitting and interpolation on the curve so you take a lot of data you embed it on some kind of interpolating manifold that enables fast but approximate judgment calls about the target space and the big idea is going to be to leverage these fast but approximate judgment calls to fight commit explosion and make program search tractable a simple analogy to understand this would be drawing a map so you take a space of discrete objects with discrete relationships that would normally require connectal search like path finding on a subway system for instance and you embed these objects uh into a latent space where you can use a continuous distance function to make fast but approximate guesses about these great relationships and this enables you to keep explosion in check while doing search and this is what the full picture looks like this is the system that we are currently working on ai is going to move towards systems that are more like programmers that approach a new task by writing software for it and when faced with a new task your programmer like metalarner will synthesize on the fly a program or model that is adapted to the task and this program will blend uh deep learning subm modules for type one sub problems like perception for instance and algorithmic modules uh for type two sub problems and these models are going to be assembled by a discrete program search system that is guided by deep learning based intuition about the structure of program space and this search process isn't done from scratch is going to leverage a global library of reusable building blocks of abstractions and that library is constantly evolving as it's learning from incoming tasks so when a new problem appears the system is going to search through this library for relevant building blocks uh and whenever in the in the course of solving a new problem you're synthesizing a new building block you're going to be uploading it back to the library much like as a software engineer if you develop a useful library for your own work you're going to put it on GitHub so that other people can reuse it and the ultimate goal here is to have an AI that can face a completely new situation and it's going to use its rich abstraction library uh to quickly assemble a working model much like a human software engineer can quickly create a piece of software to solve a new problem by leveraging existing tools in libraries and this AI is going to keep improving itself over time both by expanding its library of abstractions and also by refining its intuition about the structure of program space this system is what you are building at India our new research lab we started India because we believe that in order to dramatically accelerate scientific progress we need AI that's capable of independent invention and discovery we need AI that could expand the frontiers of knowledge not just uh operate within them and we really believe that a new form of AI is going to be key to this acceleration deep learning is great at automation it's incredibly powerful for automation but scientific discovery requires something more and our approach at Tendia is to leverage a deep learning guided uh program search to build this uh programmer like metalarner and to test our progress our first milestone is going to be to solve RKGI using a system that starts at knowing nothing at all about RKGI and you ultimately want to leverage our system for science to empower human researchers and help accelerate the timeline of science